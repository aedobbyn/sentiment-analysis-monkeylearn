---
title: "MonkeyLearn Sentiment Analysis"
output: html_document
---

In this post I'll detail how to use MonkeyLearn to conduct a seamless sentiment analysis of product reviews. I'll be examining reviews of Slack on the product review site Capterra.

The approach here will be to first scrape and tidy reviews and their associated ratings. Next, we'll feed each of the reviews to MonkeyLearn in order extract discrete opinion units from the text. Finally, we'll use a custom-trained MonkeyLearn sentiment classifier to classify each opinion unit into its primary sentiment: Negative, Neutral, or Positive.

The opinion unit approach gives us some more fine-grained control over what we're assigning sentiment to, since there can be multiple sentimetns in the same sentence. For instance, "I love Spring time but I hate the allergies that go along with it" would hopefully be broken into the units "I love Spring time" and "but I hate the allergies that go along with it" and assigned the sentiments Positive and Negative, respectively.


### Scraping the Data

First off we want to make sure that the website we're considering allows for scraping. We can consult the `robots.txt` file that typically lives at the top level of a website with the handy `robotstxt` package.

```{r setup}
library(knitr)
opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
               cache = TRUE, autodep = TRUE,     
               fig.width=12, fig.height=8,
               cache.extra = list())

options(knitr.table.format = 'html')
```


```{r}
library(here)
library(tidyverse)
library(rvest)
library(monkeylearn)
library(glue)
library(knitr)
library(dobtools)
library(tidytext)
```


```{r source_in, echo=FALSE, message=FALSE}
all_reviews_slack <- read_csv(here("data", "derived", "all_reviews_slack.csv"))
reviews_with_subratings_nested <- read_csv(here("data", "derived", "reviews_with_subratings_nested.rds"))
reviews_with_subratings_unnested <- read_csv(here("data", "derived", "capterra_slack_reviews_with_subratings_unnested.csv"))

data_dir <- glue(here::here("data", "raw"), "/")

dat <- 
  read_csv(here("data", "derived", "dat.csv")) 
```


```{r}
robotstxt::paths_allowed(
  domain = "capterra.com",
  path = "/",
  bot = "*"
)
```

Now that we know we're good to go, we can start scraping Capterra. I'll use the popular package `rvest` to do the scraping. `rvest` allows us to ask for content by a specific HTML tag or CSS class rather than grabbing all text on the page. We can find out which elements on the page to extract using the [SelectorGadget Chrome extension](). In this case, we'll need each review's rating and sub-ratings as well as its content.

We'll first save the URL where the slack reviews appear.

```{r}
slack_url <- "https://www.capterra.com/p/135003/Slack/"
```



A quick helper for stripping out extra whitespace and newlines from our HTML soup.

```{r}
strip_whitespace_newlines <- function(t) {
  out <- t %>% 
    str_replace_all("\\n", " ") %>% 
    trimws() 
  
  return(out)
}
```

The approach here

We'll wrap the `scrape_rating` function in a trycatch so that we return an `NA` if something goes wrong rather than an error.

```{r}
scrape_rating <- function(url, i) {
  out <- url %>% 
    read_html() %>% 
    html_nodes(glue("#review-{i} .overall-rating")) %>% 
    html_text() %>% 
    strip_whitespace_newlines()
  
  if (length(out) == 0) {
    out <- NA_character_
  }
  
  return(out)
}

try_scrape_rating <- possibly(scrape_rating, otherwise = NA_character_)
```


```{r, echo=FALSE}
scrape_sub_ratings <- function(url, i) {
  out <- url %>% 
    read_html() %>% 
    html_nodes(glue("#review-{i} .palm-one-half")) %>% 
    html_text() %>% 
    str_c(collapse = " ") %>% 
    strip_whitespace_newlines()
  
  if (length(out) == 0) {
    out <- NA_character_
  }
  
  return(out)
}

scrape_content <- function(url, i) {
  out <- url %>% 
    read_html() %>% 
    html_nodes(glue(".cell-review:nth-child({i}) .color-text")) %>% 
    html_text() %>% 
    strip_whitespace_newlines()
  
  if (length(out) == 0) {
    out <- NA_character_
  }
  
  return(out)
}
```


Same deal for content and sub-ratings except we use a different selector and concatenate all of the ratings with `str_c(collapse = " ")` for now. We'll break those out later into a nested list column.


```{r}
try_scrape_sub_ratings <- possibly(scrape_sub_ratings, otherwise = NA_character_)
try_scrape_content <- possibly(scrape_content, otherwise = NA_character_)
```


```{r}
try_scrape_content(slack_url, 123)
```


Now all that's left is to string these all together in the right order inside our main scraping function, `get_ratings_and_content`. In each iteration I go review by review here and grab both the rating and the review content before moving onto the next review to be absolutely sure that we're correctly matching rating and review. This approach is slower than grabbing all reviews and all ratings and matching them up afterward, but that could potentially get hairy if there are cases where we have more ratings than reviews on a page or vice versa.

Here we jitter the amount of wait time between each iteration using a uniform distribution around `sleep`, plus or minus half a second.

Throughout this post I'll be writing data out to small files in a directory so that if our loop fails somewhere along the way for any number of reasons, we won't lose all the data we've collected up to that point in the loop and can pick up again where we errored out on the first go-round. In this case, if we ask `get_ratings_and_content` to write what we've got so far, we create a new file in whatever `write_path` is set to.


```{r}
get_ratings_and_content <- function(url, review_range = 1:50,
                                    sleep = 1,
                                    write_out = TRUE, write_path = data_dir) {
  ifelse(sleep <= 1, 1, sleep)
  
  out <- tibble()
  page <- ifelse(str_detect(url, "page"), 
                 qdapRegex::ex_between(url, "page=", "&"),
                 NA_character_)
  
  for (i in review_range) {
    message(glue("Beginning scrape of page {page}, review {i}"))
    this_rating <- try_scrape_rating(url, i)
    
    this_sub_ratings <- try_scrape_sub_ratings(url, i)

    this_cont <- try_scrape_content(url, i)

    this_review <- tibble(
      rating = this_rating,
      sub_ratings = this_sub_ratings,
      content = this_cont,
      page_num = page,
      review_num = i
    ) 
    
    if (write_out == TRUE) {
      write_rds(this_review, path = glue(data_dir, "page_{page}_rating_{i}.rds"))
    }
    
    out <- out %>% 
      bind_rows(this_review)
    
    Sys.sleep(runif(1, sleep - 0.5, sleep + 0.5))
  }
  
  out <- out %>% 
    rowwise() %>% 
    mutate(
      rating_perc = ifelse(is.na(rating), NA_character_, 
                           parse(text = rating) %>% eval()) %>% as.character()
    ) %>% 
    select(page_num, review_num, rating, sub_ratings, rating_perc, content)
  
  return(out)
}

```



Let's give it a go.

```{r sample_ratings_scraped}
get_ratings_and_content(url = slack_url, 
                        review_range = 1:3)
```


The original URL, https://www.capterra.com/p/135003/Slack/, loads 99 reviews of the total 4500+ at the time of scraping. To load more, the user would hit the "Show more reviews" button at the bottom of the page. This fetches more reviews from the server, but doesn't change the URL at all. That means that just using `rvest` and this URL, we can only scrape the first 99 reviews.

However, through a bit of investigation of the "Show more reviews" button in the Chrome inspector, I saw that inside the link's `a` tag is a `data-url` parameter set to `"/gdm_reviews?page=1&product_id=135003"`. Piecing that together with our original url, https://www.capterra.com/gdm_reviews?page=1&product_id=135003 reveals the same content our original URL with slightly different styling. What this second URL offers, though, is a way to select a certain page by changing the number after `page=`.

Now we can construct the URLs for all the pages we want to iterate through. We'll take the first 45 and assume that there are 99 reviews on each page putting us at nearly the full content of our 4500 or so reviews.

```{r}
pages_want <- 1:45
slack_full_urls <- str_c("https://www.capterra.com/gdm_reviews?page=", pages_want, "&product_id=135003", sep = "")
```


Now we can wrap `get_ratings_and_content` up into something that iterates through multiple pages. The maximum number of reviews on a page is 99, so we'll set our default `review_range` to that.

In `get_ratings_and_content` we included the page and rating number in the review file name as its unique identifier.



```{r}
get_multiple_pages <- function(urls, review_range = 1:99, ...) {
  out <- tibble()
  for (u in urls) {
    this_page <- get_ratings_and_content(u, review_range = review_range, ...)
    out <- out %>% 
      bind_rows(this_page)
  }
  return(out)
}
```


We'll write our results out to our same data directory.

```{r eval=FALSE}
all_reviews_slack <- 
  slack_full_urls %>% get_multiple_pages()
```


We could also read them all in from our directory and reconstitute the dataframe.

```{r, eval=FALSE}
make_files <- function() {
  a <- glue(data_dir, "page_{1:35}")
  out <- NULL
  
  for (i in 1:99) {
    b <- c(a %>% map_chr(str_c, "_rating_", i, ".rds", sep = ""))
    out <- c(out, b)
  }
  return(out)
}

fls <- make_files()

all_reviews_slack <- map_dfr(fls, read_rds) %>% 
  unnest(page_num) %>% 
  drop_na(content)
```


<br>


### Post-Processing

Next a few quick cleaning steps. We'll `clean_content` by removing anything with more than one space.

Content consists of sub-categories like "Pros," "Cons," and "Overall". We can split those each into their own columns with a few more regexes.

```{r}
clean_content <- function(t) {
  out <- t %>% 
    t %>% 
    str_replace_all("[ ]{2,}", "")   
  
  return(out)
}


split_pro_cons <- function(t) {
  out <- t %>% 
    rowwise() %>% 
    mutate(
      content = content %>% clean_content(),
      pros = str_extract(content, "(?<=Pros:).*?(?=Cons:)")
    )
  
  out <- out %>% 
    rowwise() %>% 
    mutate(
      cons = ifelse(str_detect(content, "Overall:"), 
                    str_extract(content, "(?<=Cons:).*?(?=Overall:)"),
                    str_extract(content, "(?<=Cons:).*")),
      overall = ifelse(str_detect(content, "Overall:"),
                       str_extract(content, "(?<=Overall:).*"),
                       NA_character_)
    )
 
  return(out)
}
```


We similarly want to split sub-ratings up into their own rows. When we scraped subratings concatenated all of them into one long string. 

For each long string, we'll extract all numbers and the corresponding name of the sub-rating the rating number belongs to.

If something goes wrong and there are more subrating numbers than names or vice versa, we'll alert ourselves of this by returning a tibble that just says "length_mismatch".

```{r}
split_subratings <- function(inp) {
  
  if (is.na(inp)) {
    out <- tibble(names = NA_character_, nums = NA_character_)
    return(out)
  } 
  
  nums <- inp %>% 
    str_extract_all("([0-9 ]+\\/[0-9 ]+)") %>% 
    map(str_replace_all, "[ ]+", "") %>% 
    as_vector()
  nums <- nums[nums != ""]
  
  names <- inp %>% 
    str_split("([0-9\\. ]+\\/[0-9 ]+)") %>% 
    as_vector()
  names <- names[names != ""]
  
  if (length(nums) != length(names)) {
    out <- tibble(names = "length_mismatch", nums = "length_mismatch")
    return(out)
  }
  
  out <- tibble(names = names, nums = nums)
  return(out)
}
```



Now we can take our raw reviews and get each sub-rating into a nested dataframe inside a new `sub_ratings_split` column, which can then be unnested

```{r, eval=FALSE}
reviews_with_subratings <- all_reviews_slack %>% 
  rowwise() %>% 
  mutate(
    sub_ratings_split = split_subratings(sub_ratings) %>% list()
  ) %>% 
  select(-sub_ratings) %>% 
  drop_na(content) %>% 
  filter(!content %in% c("", " ")) %>% 
  mutate(
    page_num = as.numeric(page_num)
  )

reviews_with_subratings_unnested <- 
  reviews_with_subratings %>% 
  unnest() %>% 
  rename(
    sub_rating_category = names,
    sub_rating_rating = nums
  ) %>% 
  arrange(page_num, review_num, sub_rating_category) 
```


Let's take a peek at our first 10 rows.

```{r}
reviews_with_subratings_unnested %>% 
  slice(1:10) %>% 
  kable()
```



<br>

## Extracting Opinions


The `monkeylearn` R package can get us most of the way there. Since we're working with a custom-trained extractor and classifier, the API response is slightly different from ones that the package is set up to handle. In particular, the package's handling of `NULL` values doesn't extend perfectly to this extractor and classifier, so we'll need to do a slight modification to change the missing values into values that we can unnest correctly.

If we get a `NULL` response from our classifier we'll replace it with `replacement_classifier` and likewise for `replacement_extractor`.

```{r}
replacement_classifier <- tribble(
  ~category_id, ~probability, ~label,
  NA_character_, NA_character_, NA_character_
) %>% list()


replacement_extractor <- tribble(
  ~count, ~tag, ~entity,
  NA_character_, NA_character_, NA_character_
) 
```

We'll use these inside the unnesting helpers below. They're different because of slighly different particularities to the API response. [`dobtools::replace_x`](https://github.com/aedobbyn/dobtools/blob/master/R/replace_x.R) is just a generic "replace this NULL or 0L vector with whatever replacement I specify" function that is often useful for turning `NULL`s in API response data into `NA`s so that nested values can be unnested and tidied properly. (Incidentally, this was developed before the first version of the rOpenSci [`roomba`](https://github.com/ropenscilabs/roomba) package was released, which could have proved useful for tasks like this.)

```{r}
unnest_result_classifier <- function(df) {
  out <- df %>% 
    rowwise() %>% 
    mutate(
      res = ifelse(length(res)[[1]] == 0 || is.na(res), replacement_classifier, res) 
    ) %>% 
    unnest(res)
  
  return(out)
}

unnest_result_extractor <- function(df) {
  out <- df 
  df$res <- df$res %>% 
    map(dobtools::replace_x, replacement = replacement_extractor)
  
  out <- df %>% 
    unnest(res)
  
  return(out)
}

try_unnest_result_extractor <- safely(unnest_result_extractor)
try_unnest_result_classifier <- safely(unnest_result_classifier)
```


We wrap the usual `monkeylearn` package functions `monkey_classify` and `monkey_extract` in a trycatch using `purrr::safely` which returns a list of two things, one of which is always `NULL`; the result of the function and an error. If the function fails, we get the error and a `NULL` response and if it succeeds we get the response and a `NULL` error.

```{r}
get_classification_batch <- 
  safely(monkey_classify)

get_extraction_batch <- 
  safely(monkey_extract)
```


Rather than writing one extraction wrapper and one classifier wrapper, I combined them both into the same function below. We supply an `id` which can be either an extractor ID or a classifier ID set `type_of_problem` to either `"classification"` or `"extraction"` depending on the ID. (All classifier IDs begin with `"cl_"` and all extractor IDs begin with ``"ex_"`).

If any errors occur when we send text to the API, we log them in an error log that specifies where the error occurred and return that log along with the full response.

As of `monkeylearn` 2.0 it is possible to send batches of texts to the API and return a dataframe relating each input to its (often) multiple outputs. That allows us to include a `n_texts_per_batch` argument which we'll normally set to 200, the recommended maximum number of texts to be sent to the API at once. `monkeylearn` already takes care of rate limiting, so we don't need to put in any custom sleeps ourselves.


```{r}
write_batches <- function(df, id, dir, 
                          n_texts_per_batch,
                          start_row = 1,
                          unnest = FALSE,
                          write_out = TRUE, ...) {
  if (substr(id, 1, 3) == "cl_") {
    type_of_problem <- "classification"
  } else if (substr(id, 1, 3) == "ex_") {
    type_of_problem <- "extraction"
  } else {
    stop("Not a recognized classifier or extractor id.")
  }
  
  resp <- tibble()
  n_df_rows <- nrow(df)
  
  batch_start_row <- start_row
  batch_end_row <- batch_start_row + n_texts_per_batch
  
  error_log <- ""
  
  while(batch_start_row <= n_df_rows) {
    
    if (type_of_problem == "classification") {
      this_batch_nested <- get_classification_batch(df[batch_start_row:batch_end_row, ],
                                     col = content,
                                     classifier_id = id, 
                                     unnest = unnest)
      this_batch <- this_batch_nested$result %>% 
        try_unnest_result_classifier()
      
    } else if (type_of_problem == "extraction") {
      this_batch_nested <- get_extraction_batch(df[batch_start_row:batch_end_row, ],
                                     col = content,
                                     extactor_id = id, 
                                     unnest = unnest)
      
      this_batch <- this_batch_nested$result %>% 
        try_unnest_result_extractor()
    } 
    
    message(glue("Processed rows {batch_start_row} to {batch_end_row}."))
    
    if (is.null(this_batch_nested$error) && is.null(this_batch$error)) {
      if (write_out == TRUE) {
        write_csv(this_batch$result, 
                  glue("{dir}/{type_of_problem}_batches_rows_{batch_start_row}_to_{batch_end_row}.csv"))
      }
    
      resp <- resp %>% 
        bind_rows(this_batch$result)
      
    } else {
      error_log <- error_log %>% 
        c(glue("Error between rows {batch_start_row} and {batch_end_row}: 
               {c(this_batch_nested$error, this_batch$error)}"))
      
      message(error_log)
    }
    
    batch_start_row <- batch_start_row + n_texts_per_batch
    batch_end_row <- batch_start_row + n_texts_per_batch
    
    if (batch_end_row > n_df_rows) {
      batch_end_row <- n_df_rows
    }
  }
  
  out <- list(resp = resp, 
              error_log = error_log)
  
  return(out)
}
```




We'll take the same tack of storing each of these results in its own file (this time a CSV) in a user-specified directory. We can wrap `write_batches` in a couple helper functions with extraction- or classification-relevant results so that we don't need to specify them every time.

```{r}
write_extraction_batches <- function(df, n_texts_per_batch = 200, 
                                     dir = opinion_batches_dir, ...) {
  write_batches(df, id = extractor_id, n_texts_per_batch = n_texts_per_batch,
                dir = dir, ...)
}

write_classification_batches <- function(df, n_texts_per_batch = 200, 
                                         dir = topic_batches_dir, ...) {
  write_batches(df, id = classifier_id, n_texts_per_batch = n_texts_per_batch,
                dir = dir, ...)
}
```


We'll also need an easy way to gather up all of our result files up into a single dataframe. `gather_batches` will take a directory, create a vector of all the files that exist in it, `read_csv` in all of those files into one long list of dataframes, and then index into every element in that list to bind all of the dataframes together, rowwise.

```{r}
gather_batches <- function(dir, end_row) {
  
  fls <- fs::dir_ls(dir)
  
  list_o_batches <- 
    map(fls, 
        read_csv)
  
  out <- list_o_batches %>% 
    modify_depth(2, as.character) %>%
    bind_rows()
  
  return(out)
}
```


Cool, now we've set up everything we need to process our data and reconsitute the result. As a quick refresher, the flow here is that we'll take our scraped data, send it to the extractor to extract multiple opinion units per review, and then send each of those opinion units to the classifier to receive its multiple classifications per opinion unit.

```{r, eval=FALSE}
# Write opinion units
reviews_with_subratings_nested %>% 
  write_extraction_batches()

# Gather opinion units
opinion_batches_extracted <- 
  gather_batches(dir = opinion_batches_dir) %>% 
  rename(probability_unit = probability)

# Classify opinion units
opinion_batches_extracted %>% 
  write_classification_batches()

# Gather classifications
dat <- 
  gather_batches(dir = topic_batches_dir) %>% 
  rename(probability_sentiment = probability)
```

We could even have saved the `gather_batches` steps if everything went well with our `write_{}_batches` functions. Assigning that output to a variable also allows us to inspect the `error_log` to see if all went well.


<br>

## Analysis

Let's take a look at our data.

```{r}
dat %>% 
  head() %>% 
  kable()
```

There are only three possible sentiments for an opinion unit to have,

```{r}
dat$sentiment %>% factor() %>% levels()
```

so we can assign a number to each type of sentiment. 

```{r}
dat <- dat %>% 
  rowwise() %>% 
  mutate(
    sentiment_num = switch(sentiment, 
                           "Negative" = -1,
                           "Neutral" = 0,
                           "Positive" = 1)
  ) %>% 
  ungroup()
```


What about categories?

```{r}
dat$cateogry %>% factor() %>% levels()
```

We can see there are some categories labeled "None". It's tough to know how to interpret these, so we can filter out these rows in a new `dat_clean` dataframe. We'll also filter out low-probability sentiments and categories -- anything that the classifier is less than 55% sure is classified correctly.

```{r}
probability_cutoff <- 0.55

dat_clean <-
  dat %>% 
  filter(!is.na(probability_unit) & !is.na(probability_unit) & 
           category != "None" &
           probability_sentiment > probability_cutoff & probability_unit > probability_cutoff)
```



What is the interaction between the two main things of interest here, category and sentiment?

```{r}
sentiment_by_category <- 
  dat_clean %>% 
  group_by(category) %>% 
  summarise(
    mean_sentiment = mean(sentiment_num)
  ) %>% 
  arrange(mean_sentiment)

sentiment_by_category %>% 
  kable()
```


### Down to the word level

We can go even deeper into these reviews by splitting them into their individual words. The `tidytext` package is fantastic for this purpose. We'll use its `unnest_tokens` function to get a long dataframe of all words and then clean them up a bit by filtering out `stop_words` (a dataset included in the package) like "and" and "the". My helper [`dobtools::find_nums()`](https://github.com/aedobbyn/dobtools/blob/master/R/find_nums.R) mutates on a couple columns: one for whether the word in question is a number (i.e. can be converted to type numeric) and one for whether the word contains a number. If `is_num` is TRUE, then `contains_num` is also always TRUE.

```{r}
dat_tokens_unnested <- 
  dat_clean %>% 
  unnest_tokens(word, content) %>% 
  anti_join(stop_words, "word") %>%
  dobtools::find_nums() %>% 
  filter(contains_num == FALSE) %>% 
  add_count(word) %>% 
  rename(
    n_words_total = n
  )

dat_tokens_unnested <-  
  dat_tokens_unnested %>% 
  select(-is_num, -contains_num) %>%
  left_join(tidytext::sentiments %>% 
              rename(word_sentiment = sentiment,
                     score_sentiment = score), 
            by = "word")
```




