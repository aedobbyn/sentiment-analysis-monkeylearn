---
title: "MonkeyLearn Sentiment Analysis"
output: html_document
---

In this post I'll detail how to use MonkeyLearn to conduct a seamless sentiment analysis of product reviews.



### Scraping the Data

First off we want to make sure that the website we're considering allows for scraping. We can consult the `robots.txt` file that typically lives at the top level of a website with the handy `robotstxt` package.

```{r}
library(tidyverse)
library(rvest)
```

```{r source_in, echo=FALSE, message=FALSE}
all_reviews_slack <- read_csv(here("data", "derived", "all_reviews_slack.csv"))
reviews_with_subratings_nested <- read_csv(here("data", "derived", "reviews_with_subratings_nested.rds"))
reviews_with_subratings_unnested <- read_csv(here("data", "derived", "capterra_slack_reviews_with_subratings_unnested.csv"))

```


```{r}
robotstxt::paths_allowed(
  domain = "capterra.com",
  path = "/",
  bot = "*"
)
```

Now that we know we're good to go, we can start scraping Capterra. I'll use the popular package `rvest` to do the scraping. `rvest` allows us to ask for content by a specific HTML tag or CSS class rather than grabbing all text on the page. We can find out which elements on the page to extract using the [SelectorGadget Chrome extension](). In this case, we'll need each review's rating and sub-ratings as well as its content.


A quick helper for stripping out extra whitespace and newlines from our HTML soup.

```{r}
strip_whitespace_newlines <- function(t) {
  out <- t %>% 
    str_replace_all("\\n", " ") %>% 
    trimws() 
  
  return(out)
}
```

The approach here

We'll wrap the `scrape_rating` function in a trycatch so that we return an `NA` if something goes wrong rather than an error.

```{r}
scrape_rating <- function(url, i) {
  out <- url %>% 
    read_html() %>% 
    html_nodes(glue("#review-{i} .overall-rating")) %>% 
    html_text() %>% 
    strip_whitespace_newlines()
  
  if (length(out) == 0) {
    out <- NA_character_
  }
  
  return(out)
}

try_scrape_rating <- possibly(scrape_rating, otherwise = NA_character_)
```


Same deal for content and sub-ratings except we use a different selector and concatenate all of the ratings with `str_c(collapse = " ")` for now. We'll break those out later into a nested list column.


```{r}
try_scrape_sub_ratings <- possibly(scrape_sub_ratings, otherwise = NA_character_)
try_scrape_content <- possibly(scrape_content, otherwise = NA_character_)
```


```{r}
try_scrape_content(slack_url, 123)
```


Now all that's left is to string these all together in the right order inside our main scraping function, `get_ratings_and_content`. In each iteration I go review by review here and grab both the rating and the review content before moving onto the next review to be absolutely sure that we're correctly matching rating and review. This approach is slower than grabbing all reviews and all ratings and matching them up afterward, but that could potentially get hairy if there are cases where we have more ratings than reviews on a page or vice versa.

Here we jitter the amount of wait time between each iteration using a uniform distribution around `sleep`, plus or minus half a second.

Throughout this post I'll be writing data out to small files in a directory so that if our loop fails somewhere along the way for any number of reasons, we won't lose all the data we've collected up to that point in the loop and can pick up again where we errored out on the first go-round. In this case, if we ask `get_ratings_and_content` to write what we've got so far, we create a new file in whatever `write_path` is set to.


```{r}
get_ratings_and_content <- function(url, review_range = 1:50,
                                    sleep = 1,
                                    write_out = TRUE, write_path = data_dir) {
  ifelse(sleep <= 1, 1, sleep)
  
  out <- tibble()
  page <- ifelse(str_detect(url, "page"), 
                 qdapRegex::ex_between(url, "page=", "&"),
                 NA_character_)
  
  for (i in review_range) {
    message(glue("Beginning scrape of page {page}, review {i}"))
    this_rating <- try_scrape_rating(url, i)
    
    this_sub_ratings <- try_scrape_sub_ratings(url, i)

    this_cont <- try_scrape_content(url, i)

    this_review <- tibble(
      rating = this_rating,
      sub_ratings = this_sub_ratings,
      content = this_cont,
      page_num = page,
      review_num = i
    ) 
    
    if (write_out == TRUE) {
      write_rds(this_review, path = glue(data_dir, "page_{page}_rating_{i}.rds"))
    }
    
    out <- out %>% 
      bind_rows(this_review)
    
    Sys.sleep(runif(1, sleep - 0.5, sleep + 0.5))
  }
  
  out <- out %>% 
    rowwise() %>% 
    mutate(
      rating_perc = ifelse(is.na(rating), NA_character_, 
                           parse(text = rating) %>% eval()) %>% as.character()
    ) %>% 
    select(page_num, review_num, rating, sub_ratings, rating_perc, content)
  
  return(out)
}

```



Let's give it a go.

```{r}
get_ratings_and_content(url = slack_url, 
                        review_range = 1:3)
```


The original URL, https://www.capterra.com/p/135003/Slack/, loads 99 reviews of the total 4500+ at the time of scraping. To load more, the user would hit the "Show more reviews" button at the bottom of the page. This fetches more reviews from the server, but doesn't change the URL at all. That means that just using `rvest` and this URL, we can only scrape the first 99 reviews.

However, through a bit of investigation of the "Show more reviews" button in the Chrome inspector, I saw that inside the link's `a` tag is a `data-url` parameter set to `"/gdm_reviews?page=1&product_id=135003"`. Piecing that together with our original url, https://www.capterra.com/gdm_reviews?page=1&product_id=135003 reveals the same content our original URL with slightly different styling. What this second URL offers, though, is a way to select a certain page by changing the number after `page=`.

Now we can construct the URLs for all the pages we want to iterate through. We'll take the first 45 and assume that there are 99 reviews on each page putting us at nearly the full content of our 4500 or so reviews.

```{r}
pages_want <- 1:45
slack_full_urls <- str_c("https://www.capterra.com/gdm_reviews?page=", pages_want, "&product_id=135003", sep = "")
```


Now we can wrap `get_ratings_and_content` up into something that iterates through multiple pages. The maximum number of reviews on a page is 99, so we'll set our default `review_range` to that.

In `get_ratings_and_content` we included the page and rating number in the review file name as its unique identifier.



```{r}
get_multiple_pages <- function(urls, review_range = 1:99, ...) {
  out <- tibble()
  for (u in urls) {
    this_page <- get_ratings_and_content(u, review_range = review_range, ...)
    out <- out %>% 
      bind_rows(this_page)
  }
  return(out)
}
```


We'll write our results out to our same data directory.

```{r}
all_reviews_slack <- 
  get_multiple_pages()
```


We could also read them all in from our directory and reconstitute the dataframe.

```{r}
make_files <- function() {
  a <- glue(data_dir, "page_{1:35}")
  out <- NULL
  
  for (i in 1:99) {
    b <- c(a %>% map_chr(str_c, "_rating_", i, ".rds", sep = ""))
    out <- c(out, b)
  }
  return(out)
}

fls <- make_files()

all_reviews_slack <- map_dfr(fls, read_rds) %>% 
  unnest(page_num) %>% 
  drop_na(content)
```



### Post-Processing

Next a few quick cleaning steps. We'll `clean_content` by removing anything with more than one space.

Content consists of sub-categories like "Pros," "Cons," and "Overall". We can split those each into their own columns with a few more regexes.

```{r}
clean_content <- function(t) {
  out <- t %>% 
    t %>% 
    str_replace_all("[ ]{2,}", "")   
  
  return(out)
}


split_pro_cons <- function(t) {
  out <- t %>% 
    rowwise() %>% 
    mutate(
      content = content %>% clean_content(),
      pros = str_extract(content, "(?<=Pros:).*?(?=Cons:)")
    )
  
  out <- out %>% 
    rowwise() %>% 
    mutate(
      cons = ifelse(str_detect(content, "Overall:"), 
                    str_extract(content, "(?<=Cons:).*?(?=Overall:)"),
                    str_extract(content, "(?<=Cons:).*")),
      overall = ifelse(str_detect(content, "Overall:"),
                       str_extract(content, "(?<=Overall:).*"),
                       NA_character_)
    )
 
  return(out)
}
```


We similarly want to split sub-ratings up into their own rows. When we scraped subratings concatenated all of them into one long string. 

For each long string, we'll extract all numbers and the corresponding name of the sub-rating the rating number belongs to.

If something goes wrong and there are more subrating numbers than names or vice versa, we'll alert ourselves of this by returning a tibble that just says "length_mismatch".

```{r}
split_subratings <- function(inp) {
  
  if (is.na(inp)) {
    out <- tibble(names = NA_character_, nums = NA_character_)
    return(out)
  } 
  
  nums <- inp %>% 
    str_extract_all("([0-9 ]+\\/[0-9 ]+)") %>% 
    map(str_replace_all, "[ ]+", "") %>% 
    as_vector()
  nums <- nums[nums != ""]
  
  names <- inp %>% 
    str_split("([0-9\\. ]+\\/[0-9 ]+)") %>% 
    as_vector()
  names <- names[names != ""]
  
  if (length(nums) != length(names)) {
    out <- tibble(names = "length_mismatch", nums = "length_mismatch")
    return(out)
  }
  
  out <- tibble(names = names, nums = nums)
  return(out)
}
```



Now we can take our raw reviews

```{r}
reviews_with_subratings <- all_reviews_slack %>% 
  rowwise() %>% 
  mutate(
    sub_ratings_split = split_subratings(sub_ratings) %>% list()
  ) %>% 
  select(-sub_ratings) %>% 
  drop_na(content) %>% 
  filter(!content %in% c("", " ")) %>% 
  mutate(
    page_num = as.numeric(page_num)
  )

reviews_with_subratings_unnested <- 
  reviews_with_subratings %>% 
  unnest() %>% 
  rename(
    sub_rating_category = names,
    sub_rating_rating = nums
  ) %>% 
  arrange(page_num, review_num, sub_rating_category) 

```





