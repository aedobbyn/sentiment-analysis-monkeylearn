---
title: "MonkeyLearn Sentiment Analysis"
output:
  html_document:
    keep_md: true
    toc: false
    theme: yeti
  github_document:
    toc: false
  pdf_document:
    keep_tex: true
    toc: false
---

MonkeyLearn is a fantastic tool for performing sentiment analyses tailored to a particular use case. In this post I'll give an example of how you might use the `monkeylearn` R package to conduct a seamless sentiment analysis of consumer product reviews. I'll be examining reviews of the popular collaboration tool [Slack](https://slack.com/) on the product review site [Capterra](https://www.capterra.com/).

What do people who use Slack think of it? What aspects of the product are the most beloved and despised (at least among people who take the time to write a review of the product)? These are questions that it would be time-consuming for people to have to extract conclusions about themselves, given that there are north of 4500 reviews and counting. However, the language that people use to describe things they feel positively and negatively about follow clear enough patterns that a well-trained machine can pick them out with pretty high accuracy. We'll outsource the heavy lifting of building, training, and tweaking a model to MonkeyLearn.

**The Plan**

Specifically, MonkeyLearn will handle extracting reviews into distinct opinion units and attaching topics and sentiments to those opinion units. Most of what we have to do is shunt data back and forth between our environment and MonkeyLearn's custom machine learning modules. 

The approach here will be to first scrape and tidy reviews and their associated ratings. Next, we'll feed each of the reviews to MonkeyLearn in order to extract discrete opinion units from the text.  Finally, we'll use a custom-trained MonkeyLearn sentiment classifier to classify each opinion unit into its primary sentiment: Negative, Neutral, or Positive, as well as the category it fits into best (e.g., UI-UX, Pricing, Mobile, etc.).

The opinion unit approach gives us some more fine-grained control over what we're assigning sentiment to, since there can be multiple sentiments in the same sentence. For instance, "I love Spring time but I hate the allergies that go along with it" would hopefully be broken into the units "I love Spring time" and "but I hate the allergies that go along with it" and assigned the sentiments Positive and Negative, respectively.

Then we're left with a dataset of opinion units, each tagged with a sentiment as well as with one or many categories that we can sink our teeth into.


### Scraping the Data

First step is to collect all of the reviews of Slack that people have left on Capterra.

We'll want to make sure that the website we're considering allows for scraping. We can consult the `robots.txt` file that typically lives at the top level of a website with the handy `robotstxt` package.

```{r setup, echo=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
               cache = TRUE, autodep = TRUE,     
               fig.width=12, fig.height=8,
               cache.extra = list())

options(knitr.table.format = 'html')
```


```{r}
library(here)
library(tidyverse)
library(rvest)
library(monkeylearn)
library(glue)
library(knitr)
library(dobtools)
library(tidytext)
```


```{r source_in, echo=FALSE, message=FALSE}
all_reviews_slack <- read_csv(here("data", "derived", "all_reviews_slack.csv"))
reviews_with_subratings_nested <- read_csv(here("data", "derived", "reviews_with_subratings_nested.rds"))
reviews_with_subratings_unnested <- read_csv(here("data", "derived", "capterra_slack_reviews_with_subratings_unnested.csv"))

data_dir <- glue(here::here("data", "raw"), "/")

# dat <- 
#   read_csv(here("data", "derived", "dat.csv")) 

dat <- 
  read_rds(here("data", "derived", "dat.rds")) 
```


```{r}
robotstxt::paths_allowed(
  domain = "capterra.com",
  path = "/",
  bot = "*"
)
```

Now that we know we're good to go, we can start scraping Capterra. I'll use the popular package `rvest` to do the scraping. `rvest` allows us to ask for content that lives inside a specific HTML tag or CSS class rather than grabbing all text on the page. We can find out which elements on the page to extract using the [SelectorGadget Chrome extension](http://selectorgadget.com/).

On Capterra, reviews are parceled into chunks including the reviewer's overall rating of the product, the text of their review, and what I'm calling sub-ratings -- the ratings of certain aspects of the product, e.g. Customer Support, Ease of Use, and Value for Money. Like the overall rating, users have the option to give these aspects a rating from 1 to 5 out of 5.

<br>

![](./img/slack_review_example.jpg)

<br>

We want each review's overall rating, all of its sub-ratings, and the content of the review. 

We'll first store the URL where the Slack reviews appear,

```{r}
slack_url <- "https://www.capterra.com/p/135003/Slack/"
```

and write quick helper for stripping out extra whitespace and newlines from our HTML soup.

```{r}
strip_whitespace_newlines <- function(t) {
  out <- t %>% 
    str_replace_all("\\n", " ") %>% 
    trimws() 
  
  return(out)
}
```

**Grabbing multiple pages**

Our `slack_url` loads 99 reviews of the total 4500+ at the time of scraping. To load more, the user would hit the "Show more reviews" button at the bottom of the page. This fetches more reviews from the server, but doesn't change the URL at all. That means that just using `rvest` and this URL without involving [`RSelenium`](https://github.com/ropensci/RSelenium) or something more fancy, we can only scrape the first 99 reviews.

However, through a bit of investigation of the "Show more reviews" button in the Chrome inspector, I saw that inside the link's `a` tag is a [`data-url`](https://en.wikipedia.org/wiki/Data_URI_scheme) parameter set to `"/gdm_reviews?page=1&product_id=135003"`[^1]. 

<br>

![](./img/data_url.jpg)

<br>

We can mash that together with our original URL to get https://www.capterra.com/gdm_reviews?page=1&product_id=135003 which reveals the same content our original URL (sans the styling). The `page=1` query parameter in this URL, though, is key; it gives us a way to select a certain page by changing the number after `page=`.

Now we can construct the URLs for all the pages we want to iterate through. From a quick perusal of a few other pages, it looks like each one contains 99 reviews. We'll create URLs for the first 45, pages putting us at nearly the full content of our 4500 or so reviews.

```{r}
pages_want <- 1:45
slack_full_urls <- str_c("https://www.capterra.com/gdm_reviews?page=", pages_want, "&product_id=135003", sep = "")
```


**Defining some scraping functions**

We can specify which of the 99 reviews on a page we want to scrape with from the review's number, contained in `#review-<review_number_here>` What `scrape_rating` will do is  `#review-{i} .overall-rating` We'll wrap the `scrape_rating` function in a trycatch so that we return an `NA` if something goes wrong rather than an error.

```{r}
scrape_rating <- function(url, i) {
  out <- url %>% 
    read_html() %>% 
    html_nodes(glue("#review-{i} .overall-rating")) %>% 
    html_text() %>% 
    strip_whitespace_newlines()
  
  if (length(out) == 0) {
    out <- NA_character_
  }
  
  return(out)
}

try_scrape_rating <- possibly(scrape_rating, otherwise = NA_character_)
```


Same deal for content and sub-ratings except we use a different selector and concatenate all of the ratings with `str_c(collapse = " ")` for now. We'll break those out later into a nested list column.


```{r, echo=FALSE}
scrape_sub_ratings <- function(url, i) {
  out <- url %>% 
    read_html() %>% 
    html_nodes(glue("#review-{i} .palm-one-half")) %>% 
    html_text() %>% 
    str_c(collapse = " ") %>% 
    strip_whitespace_newlines()
  
  if (length(out) == 0) {
    out <- NA_character_
  }
  
  return(out)
}

scrape_content <- function(url, i) {
  out <- url %>% 
    read_html() %>% 
    html_nodes(glue(".cell-review:nth-child({i}) .color-text")) %>% 
    html_text() %>% 
    strip_whitespace_newlines()
  
  if (length(out) == 0) {
    out <- NA_character_
  }
  
  return(out)
}
```


We'll similarly wrap each of these in a trycatch.

```{r}
try_scrape_sub_ratings <- possibly(scrape_sub_ratings, otherwise = NA_character_)
try_scrape_content <- possibly(scrape_content, otherwise = NA_character_)
```


Let's try asking for just the 42nd review on the first page.

```{r}
try_scrape_content(slack_full_urls[1], 42)
```

**Combining scraping functions**

Now all that's left is to string these all together in the right order inside a function, `get_ratings_and_content`, that grabs a vector of reviews from a single page. We can keep track of which page we're scraping by extracting it straight from the URL itself. (If we use the original Slack URL which doesn't contain a page number, the `page` value gets an `NA`.)

In each iteration I go review by review here and grab both the rating and the review content before moving onto the next review to be absolutely sure that we're correctly matching rating and review. This approach is slower than grabbing all reviews and all ratings and matching them up afterward, but that could potentially get hairy if there are cases where we have more ratings than reviews on a page or vice versa. For fun we calculate a `rating_perc` which is the rating fraction string evaluated into its percent form (so "4/5" becomes 0.8).

Here we jitter the amount of wait time between each iteration using a uniform distribution around `sleep`, plus or minus half a second.

Throughout this post I'll be writing data out to small files in a directory so that if our loop fails somewhere along the way for any number of reasons, we won't lose all the data we've collected up to that point in the loop and can pick up again where we errored out on the first go-round. In this case, if we ask `get_ratings_and_content` to write what we've got so far, we create a new file in whatever `write_path` is set to.


```{r}
get_ratings_and_content <- function(url, review_range = 1:50,
                                    sleep = 1,
                                    write_out = TRUE, write_path = data_dir) {
  ifelse(sleep <= 1, 1, sleep)
  
  out <- tibble()
  page <- ifelse(str_detect(url, "page"), 
                 qdapRegex::ex_between(url, "page=", "&")[[1]],
                 NA_character_)
  
  for (i in review_range) {
    message(glue("Beginning scrape of page {page}, review {i}"))
    this_rating <- try_scrape_rating(url, i)
    
    this_sub_ratings <- try_scrape_sub_ratings(url, i)

    this_cont <- try_scrape_content(url, i)

    this_review <- tibble(
      rating = this_rating,
      sub_ratings = this_sub_ratings,
      content = this_cont,
      page_num = page,
      review_num = i
    ) 
    
    if (write_out == TRUE) {
      write_rds(this_review, path = glue(data_dir, "page_{page}_rating_{i}.rds"))
    }
    
    out <- out %>% 
      bind_rows(this_review)
    
    Sys.sleep(runif(1, sleep - 0.5, sleep + 0.5))
  }
  
  out <- out %>% 
    rowwise() %>% 
    mutate(
      rating_perc = ifelse(is.na(rating), NA_real_, 
                           parse(text = rating) %>% eval()) 
    ) %>% 
    select(page_num, review_num, rating, sub_ratings, rating_perc, content)
  
  return(out)
}

```



Let's give it a go with the first three reviews on the 10th page.

```{r sample_ratings_scraped, eval=TRUE, cache=TRUE}
get_ratings_and_content(url = slack_full_urls[10], 
                        review_range = 1:3) %>% 
  kable()
```


**Grabbing multiple pages**

Now that we can grab everything we want from a single page, let's wrap `get_ratings_and_content` up into something that iterates through multiple pages. The maximum number of reviews on a page is 99, so we'll set our default `review_range` to that.

In `get_ratings_and_content` we included the page and rating number in the review file name as its unique identifier. `get_multiple_pages` will write our results out to our same data directory.


```{r}
get_multiple_pages <- function(urls, review_range = 1:99, ...) {
  out <- tibble()
  for (u in urls) {
    this_page <- get_ratings_and_content(u, review_range = review_range, ...)
    out <- out %>% 
      bind_rows(this_page)
  }
  return(out)
}
```


Let's scrape all of the pages!

```{r eval=FALSE}
all_reviews_slack <- 
  slack_full_urls %>% get_multiple_pages()
```

And now we can check out what we've got:

```{r}
all_reviews_slack %>% 
  head() %>% 
  kable()
```


<br>

Since we saved all of these to their own files, we could also read them all in from our directory and reconstitute the dataframe. 

```{r, eval=FALSE}
make_files <- function(page_range) {
  a <- glue(data_dir, "page_{page_range}")
  out <- NULL
  
  for (i in page_range) {
    b <- c(a %>% map_chr(str_c, "_rating_", i, ".rds", sep = ""))
    out <- c(out, b)
  }
  return(out)
}

fls <- make_files(page_range = pages_want)

all_reviews_slack <- 
  map_dfr(fls, read_rds) %>% 
  unnest(page_num) %>% 
  drop_na(content)
```


<br>


### Post-Processing

We've got data! Next a few quick cleaning steps. Our review content often represents newlines by including a lot of extra whitespace. We'll `clean_content` by cleaning out multiple spaces.

```{r}
clean_content <- function(t) {
  out <- t %>% 
    t %>% 
    str_replace_all("[ ]{2,}", "")   
  
  return(out)
}
```



<!-- Content consists of sub-categories like "Pros," "Cons," and "Overall". We can split those each into their own columns with a few more regexes. -->

<!-- ```{r} -->
<!-- split_pro_cons <- function(t) { -->
<!--   out <- t %>%  -->
<!--     rowwise() %>%  -->
<!--     mutate( -->
<!--       content = content %>% clean_content(), -->
<!--       pros = str_extract(content, "(?<=Pros:).*?(?=Cons:)") -->
<!--     ) -->

<!--   out <- out %>%  -->
<!--     rowwise() %>%  -->
<!--     mutate( -->
<!--       cons = ifelse(str_detect(content, "Overall:"),  -->
<!--                     str_extract(content, "(?<=Cons:).*?(?=Overall:)"), -->
<!--                     str_extract(content, "(?<=Cons:).*")), -->
<!--       overall = ifelse(str_detect(content, "Overall:"), -->
<!--                        str_extract(content, "(?<=Overall:).*"), -->
<!--                        NA_character_) -->
<!--     ) -->

<!--   return(out) -->
<!-- } -->
<!-- ``` -->

**Tidying subratings**

Now let's clean up our subratings. When we scraped subratings we concatenated all of them into one long string because there wasn't an selector for each individual subrating. We want to split sub-ratings up into their own rows; for each long string, we'll extract all numbers and the corresponding name of the sub-rating the rating number belongs to (e.g. Value for Money 5/5) into a dataframe representing the same information in tidy format. We'll nest these in their own list column for later unnesting if we so choose. Rows without any sub-ratings get the appropriate NAs in both columns.

If something goes wrong and there are more subrating numbers than names or vice versa, we'll alert ourselves of this by returning a tibble that just says "length_mismatch".

```{r}
split_subratings <- function(inp) {
  
  if (is.na(inp)) {
    out <- tibble(subrating_title = NA_character_, subrating_rating = NA_character_)
    return(out)
  } 
  
  subrating_rating <- inp %>% 
    str_extract_all("([0-9 ]+\\/[0-9 ]+)") %>% 
    map(str_replace_all, "[ ]+", "") %>% 
    as_vector()
  subrating_rating <- subrating_rating[subrating_rating != ""]
  
  subrating_title <- inp %>% 
    str_split("([0-9\\. ]+\\/[0-9 ]+)") %>% 
    as_vector()
  subrating_title <- subrating_title[subrating_title != ""]
  
  if (length(subrating_rating) != length(subrating_title)) {
    out <- tibble(subrating_title = "length_mismatch", subrating_rating = "length_mismatch")
    return(out)
  }
  
  out <- tibble(subrating_title = subrating_title, subrating_rating = subrating_rating)
  return(out)
}
```



Now we can take our raw reviews and get each sub-rating into a nested dataframe inside a new `sub_ratings_split` column, which can then be unnested.

```{r, eval=FALSE}
reviews_with_subratings <- all_reviews_slack %>% 
  rowwise() %>% 
  mutate(
    sub_ratings_split = split_subratings(sub_ratings) %>% list()
  ) %>% 
  select(-sub_ratings) %>% 
  drop_na(content) %>% 
  filter(!content %in% c("", " ")) %>% 
  mutate(
    page_num = as.numeric(page_num)
  )

reviews_with_subratings_unnested <- 
  reviews_with_subratings %>% 
  unnest() %>% 
  rename(
    sub_rating_category = names,
    sub_rating_rating = nums
  ) %>% 
  arrange(page_num, review_num, sub_rating_category) 
```


Let's take a peek at our first 10 rows.

```{r}
reviews_with_subratings_unnested %>% 
  slice(1:10) %>% 
  kable()
```

Cool so in the `_nested` version of our data we've got one row per review with a nested `sub_ratings_split` column, and in the `_unnested` version we've got one as many rows per review as there are sub-ratings. We can still tell which review they each belong to because each has a unique `page_num` and `review_num` combination.

<br>

Now onto the data processing! This is where MonkeyLearn's custom modules come in. They make it simple to send data their way for extraction or classification without having to build, train, test, and iterate on a model ourselves. The MonkeyLearn modules, custom-trained on our scraped data, even provide metrics like the precision, recall, and accuracy of each particular module.

<br>

## Extracting Opinions and Classifying Topics & Sentiment

First step is to extract opinion units from our review content. Each review will have one or many opinion units. What we want to end up with is one row per opinion unit, meaning multiple rows per original content line. Next, we send each opinion unit to the API to be assigned a a single sentiment (Positive, Negative, or Neutral) and classified into one or multiple topics. For topic classifications, we'll only worry about the "leaf" topics for these purposes.[^2]

The `monkeylearn` R package can get us most of the way there. `monkeylearn` exposes two functions for processing data: `monkey_extract` for extraction and `monkey_classify` for classification. They accept either a vector of texts or a dataframe and a named column where the texts live and return a dataframe relating each row of input to its outputs (either nested in a `res` column or unnested if `unnest` is TRUE). If unnested, that result will include a few columns including the extraction/classification label as well as the probability (confidence) MonkeyLearn assigns its classification/extraction We also need to supply an API key either as a string or (preferably) stored as an environment variable.

As of `monkeylearn` 2.0 it is possible to send batches of texts to the API and return a dataframe relating each input to its (often) multiple outputs. We don't need to handle batching ourselves; `monkeylearn` will by default send 200 texts per batch (the recommended maximum number of texts to be sent to the API at once) and move onto the next batch of 200 until we get to the end of our input. Similarly, `monkeylearn` already takes care of rate limiting, so we don't need to put in any custom sleeps between requests ourselves.


**Tweaking the API response**

Since we're working with custom-trained extractors and classifiers, the API response is slightly different from responses that the package is set up to handle. In particular, the package's handling of `NULL` values doesn't extend perfectly to this extractor and classifier, so we'll need to do a slight modification to change the missing values into values that we can unnest correctly. For that reason, we'll set `unnest` to FALSE inside of `monkey_extract` and `monkey_classify` and do the unnesting ourselves.
<!-- The API returns zero-length vectors or `NULL`s for empty opinion units or when a unit wasn't classified into any categories. -->

If we get a `NULL` response from our classifier we'll replace it with `replacement_classifier` and likewise for `replacement_extractor`.

```{r}
replacement_classifier <- tribble(
  ~category_id, ~probability, ~label,
  NA_character_, NA_character_, NA_character_
) %>% list()


replacement_extractor <- tribble(
  ~count, ~tag, ~entity,
  NA_character_, NA_character_, NA_character_
) 
```

We'll use these replacements inside the unnesting helpers below. They're different because of slightly different particularities to the API response. [`dobtools::replace_x`](https://github.com/aedobbyn/dobtools/blob/master/R/replace_x.R) is just a generic "replace this NULL or 0L vector with whatever replacement I specify" function that is often useful for turning `NULL`s in API response data into `NA`s so that nested values can be unnested and tidied properly. (Incidentally, this was developed before the first version of the rOpenSci [`roomba`](https://github.com/ropenscilabs/roomba) package was released, which could have proved useful for tasks like this.)

```{r}
unnest_result_classifier <- function(df) {
  out <- df %>% 
    rowwise() %>% 
    mutate(
      res = ifelse(length(res)[[1]] == 0 || is.na(res), replacement_classifier, res) 
    ) %>% 
    unnest(res)
  
  return(out)
}

unnest_result_extractor <- function(df) {
  out <- df 
  df$res <- df$res %>% 
    map(dobtools::replace_x, replacement = replacement_extractor)
  
  out <- df %>% 
    unnest(res)
  
  return(out)
}

try_unnest_result_extractor <- safely(unnest_result_extractor)
try_unnest_result_classifier <- safely(unnest_result_classifier)
```


We wrap the usual `monkeylearn` package functions `monkey_classify` and `monkey_extract` in a trycatch using `purrr::safely` which returns a list of two things, one of which is always `NULL`; the result of the function and an error. If the function fails, we get the error and a `NULL` response and if it succeeds we get the response and a `NULL` error. We put this precaution in place because our `monkey_` function could throw an error for a few reasons; our request to MonkeyLearn could fail because either we or the server lose network connectivity, we receive a response we didn't expect and try to perform some operation on it that fails, or for some other unforeseen reason.

```{r}
get_classification_batch <- 
  safely(monkey_classify)

get_extraction_batch <- 
  safely(monkey_extract)
```

Let's test what happens something fails.

```{r}
safe_return <- 
  NULL %>% get_classification_batch()

safe_return
```

The function executes successfully and we get a record of the error message.

**Batching input to the MonkeyLearn API**

Now a function wrap up `monkey_classify` and `monkey_extract`. Rather than writing one extraction wrapper and one classifier wrapper, I combined them both into the same function below. We supply an `id` which can be either an extractor ID or a classifier ID set `type_of_problem` to either `"classification"` or `"extraction"` depending on the ID. (All classifier IDs begin with `"cl_"` and all extractor IDs begin with ``"ex_"`).

If any errors occur when we send text to the API, we log them in an error log that specifies where the error occurred and return that log along with the full response.

We'll take the same tack of storing each of these results in its own file (this time a CSV) in a user-specified directory.


```{r}
write_batches <- function(df, id, dir, 
                          n_texts_per_batch,
                          start_row = 1,
                          unnest = FALSE,
                          write_out = TRUE, ...) {
  if (substr(id, 1, 3) == "cl_") {
    type_of_problem <- "classification"
  } else if (substr(id, 1, 3) == "ex_") {
    type_of_problem <- "extraction"
  } else {
    stop("Not a recognized classifier or extractor id.")
  }
  
  resp <- tibble()
  n_df_rows <- nrow(df)
  
  batch_start_row <- start_row
  batch_end_row <- batch_start_row + n_texts_per_batch
  
  error_log <- ""
  
  while(batch_start_row <= n_df_rows) {
    
    if (type_of_problem == "classification") {
      this_batch_nested <- get_classification_batch(df[batch_start_row:batch_end_row, ],
                                     col = content,
                                     classifier_id = id, 
                                     unnest = unnest)
      this_batch <- this_batch_nested$result %>% 
        try_unnest_result_classifier()
      
    } else if (type_of_problem == "extraction") {
      this_batch_nested <- get_extraction_batch(df[batch_start_row:batch_end_row, ],
                                     col = content,
                                     extactor_id = id, 
                                     unnest = unnest)
      
      this_batch <- this_batch_nested$result %>% 
        try_unnest_result_extractor()
    } 
    
    message(glue("Processed rows {batch_start_row} to {batch_end_row}."))
    
    if (is.null(this_batch_nested$error) && is.null(this_batch$error)) {
      if (write_out == TRUE) {
        write_csv(this_batch$result, 
                  glue("{dir}/{type_of_problem}_batches_rows_{batch_start_row}_to_{batch_end_row}.csv"))
      }
    
      resp <- resp %>% 
        bind_rows(this_batch$result)
      
    } else {
      error_log <- error_log %>% 
        c(glue("Error between rows {batch_start_row} and {batch_end_row}: 
               {c(this_batch_nested$error, this_batch$error)}"))
      
      message(error_log)
    }
    
    batch_start_row <- batch_start_row + n_texts_per_batch
    batch_end_row <- batch_start_row + n_texts_per_batch
    
    if (batch_end_row > n_df_rows) {
      batch_end_row <- n_df_rows
    }
  }
  
  out <- list(resp = resp, 
              error_log = error_log)
  
  return(out)
}
```


We can wrap `write_batches` in a couple helper functions with extraction- or classification-relevant results so that we don't need to specify them every time.

```{r}
write_extraction_batches <- function(df, n_texts_per_batch = 200, 
                                     dir = opinion_batches_dir, ...) {
  write_batches(df, id = extractor_id, n_texts_per_batch = n_texts_per_batch,
                dir = dir, ...)
}

write_classification_batches <- function(df, n_texts_per_batch = 200, 
                                         dir = topic_batches_dir, ...) {
  write_batches(df, id = topic_id, n_texts_per_batch = n_texts_per_batch,
                dir = dir, ...)
}
```


For example:

```{r, eval=FALSE}
reviews_with_subratings_nested[123, ] %>% 
  write_extraction_batches(n_texts_per_batch = 1 , write_out = FALSE) %>% 
  write_classification_batches(n_texts_per_batch = 1, write_out = FALSE)
```



After we write all the results to their individual files, we'll want an easy way to gather up all of our result files up into a single dataframe. `gather_batches` will take a directory, create a vector of all the files that exist in it, `read_csv` in all of those files into one long list of dataframes, and then index into every element in that list to bind all of the dataframes together, rowwise.

```{r}
gather_batches <- function(dir, end_row) {
  
  fls <- fs::dir_ls(dir)
  
  list_o_batches <- 
    map(fls, 
        read_csv)
  
  out <- list_o_batches %>% 
    modify_depth(2, as.character) %>%
    bind_rows()
  
  return(out)
}
```


**Sending text the API and gathering it all up again**

Cool, now we've set up everything we need to process our data and reconstitute the result. As a quick refresher, the flow here is that we'll take our scraped data, send it to the extractor to extract multiple opinion units per review, and then send each of those opinion units to the classifier to receive its sentiment rating and multiple classifications per opinion unit.

```{r, eval=FALSE}
# Write opinion units
reviews_with_subratings_nested %>% 
  write_extraction_batches()

# Gather opinion units
opinion_batches_extracted <- 
  gather_batches(dir = opinion_batches_dir) %>% 
  rename(probability_unit = probability)

# Classify opinion units
opinion_batches_extracted %>% 
  write_classification_batches()

# Gather classifications
dat <- 
  gather_batches(dir = topic_batches_dir) %>% 
  rename(probability_sentiment = probability)
```

We could even have skipped the `gather_batches` steps if everything went well with our `write_{}_batches` functions. Assigning that output to a variable also allows us to inspect the `error_log` to see if all went well.


<br>

## Analysis

We've got data! `r emo::ji("tada") ` Let's take a look at it. 

```{r, dependson="source_in"}
dat %>% 
  slice(1:20) 
```

Since there are multiple rows per review, we'll want a unique identifier for each. Each `page_num, review_num` combination represents a unique review. We could hash these two values but since we have the benefit of knowing that the reviews happen in chronological order, it seems better to number them, starting with 1 for our oldest review.

For good measure I also created a `doc_identifier` by smushing together the page and review number.

```{r}
dat <- dat %>%
  mutate(
    doc_identifier = str_c("r", review_num, "p", page_num, sep = "_")
  )

uuids <- dat %>% 
  arrange(page_num, review_num) %>%
  nest(-doc_identifier) %>% 
  mutate(doc_uuid = nrow(.) - row_number() + 1) %>% 
  select(-data)

dat <- dat %>% 
  left_join(uuids) 
```


There are only three possible sentiments for an opinion unit to have,

```{r}
dat$sentiment %>% factor() %>% levels()
```

so we can assign a number to each type of sentiment in order to be able to represent them on an ordinal scale. 


```{r numerise_sentiment, dependson="source_in"}
dat <- dat %>% 
  rowwise() %>% 
  mutate(
    sentiment_num = switch(sentiment, 
                           "Negative" = -1,
                           "Neutral" = 0,
                           "Positive" = 1)
  ) %>% 
  ungroup()
```



What about categories?

```{r}
dat$category %>% factor() %>% levels()
```

We can see there are some categories labeled "None". It's tough to know how to interpret these, so we can filter out these rows in a new `dat_clean` dataframe. We'll also filter out low-probability sentiments and categories -- anything that the classifier is less than 55% sure is classified correctly.

```{r}
probability_cutoff <- 0.55

dat_clean <-
  dat %>% 
  filter(!is.na(probability_unit) & !is.na(probability_unit) & 
           category != "None" &
           probability_sentiment > probability_cutoff & probability_unit > probability_cutoff)
```

After cleaning, we've got `r length(unique(dat_clean$doc_uuid))` unique opinion units to work with, each with a single sentiment and multiple classifications.


**Initial exploring**

Now let's get the lay of the land by seeing what the breakdown of sentiments is overall.

```{r}
sentiment_breakdown <- 
  dat_clean %>% 
  group_by(sentiment) %>% 
  count() %>% 
  rename(by_sentiment = n) %>% 
  ungroup() %>% 
  mutate(total = sum(by_sentiment),
         sentiment_prop = by_sentiment/total)

ggplot(sentiment_breakdown) +
  geom_bar(aes(sentiment, sentiment_prop, fill = sentiment), position = "dodge", stat = "identity") + 
  geom_text(aes(sentiment, sentiment_prop + 0.03, label = paste0(round(sentiment_prop*100, digits = 2), "%")),
            fontface = "italic", size = 3) +
  scale_y_continuous(labels = scales::percent) +
  ggtitle("Sentiment Breakdown, Overall") +
  labs(x = "Sentiment", y = "Percent", fill = "Sentiment") +
  theme_bw()
```

We can see there are very few reviews that have a Neutral sentiment, which is useful for us. It's easier to draw conclusions about the strengths and weaknesses of a product when most of the feedback is either definitively positive or negative. (That could also be a reflection of the tendency of reviewers to feel more strongly about the product they're reviewing than the general user base. But whether or not these reviews are an unbiased reflection of most users' true feelings about the product is neither here nor there `r emo::ji("laughing")`.)


What is the interaction between the two main things of interest here, category and sentiment? Let's get a summary of the mean sentiment (based off of our numerical representation of sentiment) for opinion units that have been classified into each category.


```{r}
sentiment_by_category <- 
  dat_clean %>% 
  group_by(category) %>% 
  summarise(
    mean_sentiment = mean(sentiment_num)
  ) %>% 
  arrange(desc(mean_sentiment))

sentiment_by_category %>% 
  kable()
```

`r sentiment_by_category[which(sentiment_by_category$mean_sentiment == min(sentiment_by_category$mean_sentiment)), ]$category` gets the lowest overall sentiment, whereas `r sentiment_by_category[which(sentiment_by_category$mean_sentiment == max(sentiment_by_category$mean_sentiment)), ]$category` gets the highest.

```{r}
ggplot(sentiment_by_category) +
  geom_bar(aes(fct_reorder(category, mean_sentiment), mean_sentiment), stat = "identity") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Sentiment by Category") +
  labs(x = "Category", y = "Sentiment")
```


Are the categories that often have a negative sentiment categories that people tend to mention often in their reviews, or are they less frequent?

```{r}
category_freq <- 
  dat_clean %>% 
  group_by(category) %>% 
  count(sort = TRUE) %>% 
  rename(
    n_opinion_units = n
  )

ggplot(category_freq) +
  geom_bar(aes(fct_reorder(category, n_opinion_units), n_opinion_units), stat = "identity") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Category Mentions") +
  labs(x = "Category", y = "N")
```


Now we can weight the category sentiment by the number of times it occurs in an opinion unit.

```{r}
sentiment_by_category_weighted <- 
  sentiment_by_category %>% 
  left_join(category_freq, by = "category") %>% 
  mutate(
    weighted_sentiment = mean_sentiment*n_opinion_units
  ) %>% 
  arrange(desc(weighted_sentiment))

sentiment_by_category_weighted %>% 
  head() %>% 
  kable()
```


```{r, echo=FALSE}
ggplot(sentiment_by_category_weighted) +
  geom_bar(aes(fct_reorder(category, weighted_sentiment), weighted_sentiment), stat = "identity") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Category Sentiment, Weighted") +
  labs(x = "Category", y = "Weighted Sentiment")
```


What about ratings? How do those line up with sentiments we assigned?

```{r}
ratings_by_sentiment <- 
  dat_clean %>% 
  group_by(sentiment) %>% 
  summarise(
    mean_rating = mean(rating_perc %>% as.numeric(), na.rm = TRUE)
  )
```

There is very little difference in overall ratings of the product. (It's important to remember that there is a one:many relationship between ratings and opinion units here; each review gets a at most single rating, but reviews are later parceled into multiple opinions.)

This indicates that despite critiques and a good chunk of negative opinion units, most overall reviews remain positive.

```{r, include=FALSE}
ggplot(dat_clean) +
  geom_jitter(aes(sentiment_num, as.numeric(rating_perc))) +
  geom_smooth(aes(sentiment_num, as.numeric(rating_perc)), method = "lm")
```

```{r}
ggplot(ratings_by_sentiment) +
  geom_bar(aes(sentiment, mean_rating), stat = "identity") +
  coord_cartesian(ylim = c(0.85, 1))
```


We can dig into sub_ratings

```{r, include=FALSE}
subrated_dat <- 
  dat_clean %>% 
  unnest(sub_ratings_split) %>% 
  mutate(
    subrating_rating = subrating_rating %>% as.numeric()
  ) %>% 
  select(doc_uuid, subrating_title, subrating_rating, sentiment, sentiment_num, rating_perc, opinion_unit)
```

```{r}
parsed_subratings <-
  reviews_with_subratings_unnested %>% 
  rowwise() %>% 
  mutate(subrating_num = 
         ifelse(is.na(sub_rating_rating), NA, 
                           parse(text = sub_rating_rating) %>% eval()) 
  ) 

parsed_subratings_summary <- 
  parsed_subratings %>% 
  drop_na(subrating_num, sub_rating_category) %>% 
  group_by(sub_rating_category) %>%
  summarise(
    mean_subrating = mean(subrating_num)
  )

parsed_subratings_summary %>% 
  dobtools::cap_df() %>% 
  kable()
```


How do these sub-ratings match up with category ratings we calculated earlier?

```{r}
parsed_subratings_summary$alias <- c("Customer Support", "Ease of Use", "General", "Pricing")

parsed_subratings_summary %>% 
  left_join(sentiment_by_category, 
            by = c("alias" = "category"))
```





<br>


### Down to the word level

Now that we have classifications for each opinion units, we can see how the individual words in opinion units map to the sentiment and category classification they were assigned, and maybe gain some more granular insight about what people like and dislike about the product.

The `tidytext` package is fantastic for this purpose. We'll use its `unnest_tokens` function to get a long dataframe of all words and then clean them up a bit by filtering out `stop_words` (a dataset included in the package) like "and" and "the". My helper [`dobtools::find_nums()`](https://github.com/aedobbyn/dobtools/blob/master/R/find_nums.R) mutates on a couple columns: one for whether the word in question is a number (i.e. can be converted to type numeric) and one for whether the word contains a number. If `is_num` is TRUE, then `contains_num` is also always TRUE.

```{r}
dat_tokenized <- 
  dat_clean %>% 
    nest(-content, -doc_uuid) %>% 
    unnest_tokens(word, content) %>% 
    anti_join(stop_words, "word") %>%
    dobtools::find_nums() %>% 
    filter(contains_num == FALSE)
```

The `tidytext` package also includes a `sentiments` dataset which we can join on our words to get a classification of the words' sentiment in three different lexicons as well as its score on a scale of -5 (negative) to 5 (positive). (`?tidytext::sentiments` for a full explanation of the dataset.) For example:

```{r}
sentiments %>% filter(word == "yucky")
```

and 

```{r}
sentiments %>% filter(word == "yummy")
```

(Don't ask me why yummy is more positive than yucky is negative `r emo::ji("laughing")`.) Anyway, let's join this on our data set by "word".


```{r}
dat_tokenized <-  
  dat_tokenized %>% 
  select(-is_num, -contains_num) %>%
  left_join(tidytext::sentiments %>% 
              rename(word_sentiment = sentiment,
                     score_sentiment = score), 
            by = "word")
```



If we're interested in doing an analysis of words that belong to opinion units that were tagged with certain categories or sentiments, we're going to need back our measure of each of those, which were labeled at the opinion unit level. Luckily we can unnest our `dat_tokenized_tfidf` dataframe in place of doing any joining.

```{r}
dat_tokenized_unnested <- 
  dat_tokenized %>% 
  unnest()
```



Do the words that make up an opinion unit have a significant effect on the sentiment it's assigned by MonkeyLearn?

```{r}
lm(data = dat_tokenized_unnested %>% drop_na(sentiment_num, score_sentiment), 
   sentiment_num ~ score_sentiment) %>% 
  summary() %>% 
  tidy()
```

Yes -- words with more positive sentiments tend to occur in more positive opinion units.


As a control, we can check whether words that appear at the beginning of the alphabet tend to get higher sentiment scores. (I don't know of a reason to suspect this might be the case which is why I'm treating it as a control but maybe there is a psycholinguist out there who can set me straight?)

We'll extract just the first letter of the word and assign it the number of the alphabet from 1 to 24.

```{r}
assign_number <- function(l) {
  if (length(l) == 0 || ! l %in% letters) {
    n <- NA_integer_
  } else {
    n <- which(letters == l)
  
  }
  n
}

try_assign_number <- possibly(assign_number, otherwise = NA_integer_)
```


```{r}
dat_tokenized_first_letter <- 
  dat_tokenized %>% 
  rowwise() %>% 
  mutate(
    first_letter = substr(word, 1, 1),
    first_letter_num = try_assign_number(first_letter)
  ) 
```

And then plot the word's sentiment as scored on the `AFINN` scale. The dashed horizontal line represents the mean sentiment score for words in our data set.

```{r, echo=FALSE}
ggplot(dat_tokenized_first_letter) +
  # geom_smooth(aes(first_letter_num, sentiment_num),
  #             colour = "blue") +
  geom_smooth(aes(first_letter_num, score_sentiment),
              colour = "blue") +
  # geom_smooth(aes(first_letter_num, score_sentiment),
  #             colour = "dark green", method = "lm",
  #             linetype = "dashed", se = FALSE) +
  geom_hline(data = dat_tokenized_first_letter, yintercept = mean(dat_tokenized_first_letter$score_sentiment, na.rm = TRUE), linetype = "dashed") +
  labs(x = "First letter", y = "Sentiment") +
  ggtitle("Relationship between a word's first letter and its sentiment") +
  theme_bw()
```


What about the statistical relationship?

```{r}
lm(data = dat_tokenized_first_letter %>% drop_na(first_letter_num, score_sentiment), 
   first_letter_num ~ score_sentiment) %>% 
  summary() %>% 
  tidy()
```

Not significant, as we might expect.


<br>

```{r, echo=FALSE}
replace_y <- function (x, replacement = NA_character_) {
  if (is.null(x) || length(x) == 0 || length(x[[1]]) == 0 || is.null(x[[1]])) {
    replacement
  }
  else {
    x
  }
}
```

We might be interested in phrases that follow specific words like "use" or "don't use". Here we ask for everything after `word` and up until the first period.

We might also want to pull out a category or categories if they exist in the phrase. To that end we'll make a regex for all of the categories MonkeyLearn has identified (except Other which seems uninteresting):

```{r}
category_reg <- 
  dat_clean$category[-which(dat_clean$category == "Other")] %>%
  tolower() %>% unique() %>% str_c(collapse = "|") 
```


We can make something reusable like:

```{r}
search_for <- function(df = dat_clean, col = content, word = "love", append_the = FALSE) {
  word_capped <- 
    dobtools::simple_cap(word)
  
  q_col = enquo(col)
  
  look_for <- ifelse(append_the == TRUE,
                     glue("{word} the |{word_capped} the "),
                     glue("{word} |{word_capped} "))
  
  out <- 
    df %>% 
    filter(
      str_detect(!!q_col, look_for)
    ) %>% 
    distinct(!!q_col) %>% 
    rowwise() %>% 
    mutate(
      phrase = str_extract(!!q_col, glue("(?<={look_for}).+$")) %>% 
        str_replace_all("(?<=\\.).*", ""),
      phrase_categories = str_extract_all(phrase, category_reg) %>% 
        replace_y() %>% as_vector() %>% unique() %>% str_c(collapse = ", ")
    ) 
  
  return(out)
}
```


We can ask for our word always followed by a "the" so that we know our `phrase` will start with a noun that our `word` is referring to.

```{r}
search_for(word = "love", append_the = TRUE) %>% 
  dobtools::replace_na_df() %>% 
  head() %>% 
  kable()
```

By default we won't append "the" after `word`. We can filter to just opinion units that contain our word and then the name of one of our categories following it.

```{r}
search_for(word = "use") %>% 
  drop_na(phrase_categories) %>% 
  dobtools::replace_na_df() %>% 
  head() %>% 
  kable()
```


```{r}
search_for(word = "want") %>% 
  drop_na(phrase_categories) %>% 
  dobtools::replace_na_df() %>% 
  head() %>% 
  kable()
```




#### TF-IDF

Term frequency inverse document frequency is a measure of how often a word appears in a given document (here, a review) as opposed to overall, in all of the documents. That can give us a sense of how important it is to a given document as compared to a baseline of all words used in the entire corpus.

What we can learn from TF-IDF is, for instance, what words do people often use in reviews when they're talking about a specific aspect of the product that they tend to use less frequently when talking about other aspects of the product?

To get every word's TF-IDF, we need to get within-document counts of each word. We'll also count how often the word is used in all reviews.

```{r}
dat_tokenized_counts <- 
  dat_tokenized %>% 
  add_count(word) %>% 
  rename(
    n_words_total = n
  ) %>% 
  group_by(doc_uuid) %>% 
  add_count(word) %>% 
  rename(
    n_words_this_doc = n
  )  %>% 
  ungroup() 
```


Now we can ask `tidytext` to do a mutate and attach the `tf_idf` of each word to our dataframe.

```{r}
dat_tokenized_tfidf <- 
  dat_tokenized_counts %>% 
  bind_tf_idf(word, doc_uuid, n_words_this_doc)
```

A question we might be interested in is: which words are most distinctive to opinion units tagged with each category? For this purpose we can treat each category as its own document (rather than each review as its own document).

```{r}
category_tfidf <-
  dat_tokenized_unnested %>% 
  group_by(category) %>% 
  add_count(word) %>% 
  rename(
    n_words_this_category = n
  )  %>% 
  ungroup() %>% 
  bind_tf_idf(word, category, n_words_this_category) %>% 
  select(word, category, tf_idf, opinion_unit, sentiment)

category_tfidf_maxes <- 
  category_tfidf %>% 
  unnest() %>% 
  group_by(category) %>% 
  filter(tf_idf == max(tf_idf)) %>% 
  select(word, sentiment, category, tf_idf) %>% 
  distinct(word, category, tf_idf) %>% 
  arrange(category, word) 

category_tfidf_maxes %>% 
  dobtools::cap_df() %>% 
  kable()
```

```{r, include=FALSE}
sentiment_tfidf <-
  dat_tokenized_unnested %>% 
  group_by(sentiment) %>% 
  add_count(word) %>% 
  rename(
    n_words_this_sentiment = n
  )  %>% 
  ungroup() %>% 
  bind_tf_idf(word, sentiment, n_words_this_sentiment) %>% 
  select(word, sentiment, tf_idf, opinion_unit, category)

sentiment_tfidf_maxes <- 
  sentiment_tfidf %>% 
  unnest() %>% 
  group_by(sentiment) %>% 
  filter(tf_idf == max(tf_idf)) %>% 
  # filter(tf_idf > mean(tf_idf) + sd(tf_idf)*2) %>% 
  select(word, sentiment, category, tf_idf) %>% 
  distinct(word, sentiment, tf_idf) %>% 
  arrange(sentiment, word) 

sentiment_tfidf_maxes %>% 
  dobtools::cap_df() %>% 
  kable()
```




#### Going Negative

Let's focus on the places where Slack might want to improve. Pricing is more self-explanatory, so I'll focus on Performance-Quality-Reliability and Notifications.


```{r}
problem_categories <- 
  dat_clean %>% 
  filter(category %in% c("Performance-Quality-Reliability", "Notifications") &
           sentiment == "Negative")
```


```{r}
pqr_complaints <- 
  problem_categories %>% 
  mutate(review_num = row_number()) %>% 
  unnest_tokens(word, content) %>% 
  filter(category == "Performance-Quality-Reliability") %>% 
  anti_join(stop_words, by = "word") %>% 
  count(word, sort = TRUE)
```

Is the desktop app or mobile app mentioned more in P-Q-R complaints?

```{r}
pqr_complaints %>% 
  filter(word %in% c("desktop", "mobile"))
```


How does that compare to the base rate of the mentions of desktop and mobile?

```{r}
dat_tokenized %>% 
  filter(word %in% c("desktop", "mobile")) %>% 
  group_by(word) %>% 
  count()
```

So interestingly, even though mobile is mentioned more often than desktop in reviews, most of the P-Q-R complaints seem to be about the desktop app. 


This is an area where companies can compare their own metrics to the same data scraped from reviews of other companies and trained using the same or very similar modules.


## Wrap-Up

Here we've built a relatively straightforward pipeline for an analysis of web data. We grab and clean our raw data, feed it to MonkeyLearn for extraction and classification, and then analyze the results. MonkeyLearn allows us to abstract out the machine learning and plug into a simple and reliable API. 

Thanks and happy coding!



[^1]: Shoutout to some co-detective work with [Josh](https://www.fieldmuseum.org/blog/open-tree-life-toward-global-synthesis-phylogenetic-knowledge) [Stevens-Stein](https://github.com/jstevensstein)

[^2]: What I mean by that is: this particular topic classifier has a tree-like structure where each leaf belong to a single parent. MonkeyLearn first classifies each text into a top-level supercategory, one of: App-Software, Service, or Other. Once a text is classified at this first level, it then gets classified into one or more children in that supercategory. For instance, the children of App-Software are: Characteristics, Devices-Web, and Features. Finally, each of these has its own children. Take Characteristics: its children, which are leaf categories or terminal nodes for the classifier are Ease of Use, Integrations, Performance-Quality-Reliability, and UI-UX. This means that if a text doesn't appear to reference App-Software, the grandparent of UI-UX, it won't have a chance of being classified as UI-UX. This scheme of course doesn't preclude a text from being classified under multiple terminal nodes.

