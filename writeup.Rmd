---
title: "MonkeyLearn Sentiment Analysis"
output: html_document
---

In this post I'll detail how to use MonkeyLearn to conduct a seamless sentiment analysis of product reviews. I'll be examining reviews of Slack on the product review site Capterra.

The approach here will be to first scrape and tidy reviews and their associated ratings. Next, we'll feed each of the reviews to MonkeyLearn in order extract discrete opinion units from the text. Finally, we'll use a custom-trained MonkeyLearn sentiment classifier to classify each opinion unit into its primary sentiment: Negative, Neutral, or Positive.

The opinion unit approach gives us some more fine-grained control over what we're assigning sentiment to, since there can be multiple sentimetns in the same sentence. For instance, "I love Spring time but I hate the allergies that go along with it" would hopefully be broken into the units "I love Spring time" and "but I hate the allergies that go along with it" and assigned the sentiments Positive and Negative, respectively.


### Scraping the Data

First off we want to make sure that the website we're considering allows for scraping. We can consult the `robots.txt` file that typically lives at the top level of a website with the handy `robotstxt` package.

```{r setup}
library(knitr)
opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
               cache=TRUE, autodep = TRUE,     # Cache and auto-dependson
               fig.width=12, fig.height=8,
               cache.extra = list())
# Format for kables
options(knitr.table.format = 'html')
```


```{r}
library(here)
library(tidyverse)
library(rvest)
library(glue)
library(knitr)
library(dobtools)
```


```{r source_in, echo=FALSE, message=FALSE}
all_reviews_slack <- read_csv(here("data", "derived", "all_reviews_slack.csv"))
reviews_with_subratings_nested <- read_csv(here("data", "derived", "reviews_with_subratings_nested.rds"))
reviews_with_subratings_unnested <- read_csv(here("data", "derived", "capterra_slack_reviews_with_subratings_unnested.csv"))

data_dir <- glue(here::here("data", "raw"), "/")
```


```{r}
robotstxt::paths_allowed(
  domain = "capterra.com",
  path = "/",
  bot = "*"
)
```

Now that we know we're good to go, we can start scraping Capterra. I'll use the popular package `rvest` to do the scraping. `rvest` allows us to ask for content by a specific HTML tag or CSS class rather than grabbing all text on the page. We can find out which elements on the page to extract using the [SelectorGadget Chrome extension](). In this case, we'll need each review's rating and sub-ratings as well as its content.

We'll first save the URL where the slack reviews appear.

```{r}
slack_url <- "https://www.capterra.com/p/135003/Slack/"
```



A quick helper for stripping out extra whitespace and newlines from our HTML soup.

```{r}
strip_whitespace_newlines <- function(t) {
  out <- t %>% 
    str_replace_all("\\n", " ") %>% 
    trimws() 
  
  return(out)
}
```

The approach here

We'll wrap the `scrape_rating` function in a trycatch so that we return an `NA` if something goes wrong rather than an error.

```{r}
scrape_rating <- function(url, i) {
  out <- url %>% 
    read_html() %>% 
    html_nodes(glue("#review-{i} .overall-rating")) %>% 
    html_text() %>% 
    strip_whitespace_newlines()
  
  if (length(out) == 0) {
    out <- NA_character_
  }
  
  return(out)
}

try_scrape_rating <- possibly(scrape_rating, otherwise = NA_character_)
```


```{r, echo=FALSE}
scrape_sub_ratings <- function(url, i) {
  out <- url %>% 
    read_html() %>% 
    html_nodes(glue("#review-{i} .palm-one-half")) %>% 
    html_text() %>% 
    str_c(collapse = " ") %>% 
    strip_whitespace_newlines()
  
  if (length(out) == 0) {
    out <- NA_character_
  }
  
  return(out)
}

scrape_content <- function(url, i) {
  out <- url %>% 
    read_html() %>% 
    html_nodes(glue(".cell-review:nth-child({i}) .color-text")) %>% 
    html_text() %>% 
    strip_whitespace_newlines()
  
  if (length(out) == 0) {
    out <- NA_character_
  }
  
  return(out)
}
```


Same deal for content and sub-ratings except we use a different selector and concatenate all of the ratings with `str_c(collapse = " ")` for now. We'll break those out later into a nested list column.


```{r}
try_scrape_sub_ratings <- possibly(scrape_sub_ratings, otherwise = NA_character_)
try_scrape_content <- possibly(scrape_content, otherwise = NA_character_)
```


```{r}
try_scrape_content(slack_url, 123)
```


Now all that's left is to string these all together in the right order inside our main scraping function, `get_ratings_and_content`. In each iteration I go review by review here and grab both the rating and the review content before moving onto the next review to be absolutely sure that we're correctly matching rating and review. This approach is slower than grabbing all reviews and all ratings and matching them up afterward, but that could potentially get hairy if there are cases where we have more ratings than reviews on a page or vice versa.

Here we jitter the amount of wait time between each iteration using a uniform distribution around `sleep`, plus or minus half a second.

Throughout this post I'll be writing data out to small files in a directory so that if our loop fails somewhere along the way for any number of reasons, we won't lose all the data we've collected up to that point in the loop and can pick up again where we errored out on the first go-round. In this case, if we ask `get_ratings_and_content` to write what we've got so far, we create a new file in whatever `write_path` is set to.


```{r}
get_ratings_and_content <- function(url, review_range = 1:50,
                                    sleep = 1,
                                    write_out = TRUE, write_path = data_dir) {
  ifelse(sleep <= 1, 1, sleep)
  
  out <- tibble()
  page <- ifelse(str_detect(url, "page"), 
                 qdapRegex::ex_between(url, "page=", "&"),
                 NA_character_)
  
  for (i in review_range) {
    message(glue("Beginning scrape of page {page}, review {i}"))
    this_rating <- try_scrape_rating(url, i)
    
    this_sub_ratings <- try_scrape_sub_ratings(url, i)

    this_cont <- try_scrape_content(url, i)

    this_review <- tibble(
      rating = this_rating,
      sub_ratings = this_sub_ratings,
      content = this_cont,
      page_num = page,
      review_num = i
    ) 
    
    if (write_out == TRUE) {
      write_rds(this_review, path = glue(data_dir, "page_{page}_rating_{i}.rds"))
    }
    
    out <- out %>% 
      bind_rows(this_review)
    
    Sys.sleep(runif(1, sleep - 0.5, sleep + 0.5))
  }
  
  out <- out %>% 
    rowwise() %>% 
    mutate(
      rating_perc = ifelse(is.na(rating), NA_character_, 
                           parse(text = rating) %>% eval()) %>% as.character()
    ) %>% 
    select(page_num, review_num, rating, sub_ratings, rating_perc, content)
  
  return(out)
}

```



Let's give it a go.

```{r}
get_ratings_and_content(url = slack_url, 
                        review_range = 1:3)
```


The original URL, https://www.capterra.com/p/135003/Slack/, loads 99 reviews of the total 4500+ at the time of scraping. To load more, the user would hit the "Show more reviews" button at the bottom of the page. This fetches more reviews from the server, but doesn't change the URL at all. That means that just using `rvest` and this URL, we can only scrape the first 99 reviews.

However, through a bit of investigation of the "Show more reviews" button in the Chrome inspector, I saw that inside the link's `a` tag is a `data-url` parameter set to `"/gdm_reviews?page=1&product_id=135003"`. Piecing that together with our original url, https://www.capterra.com/gdm_reviews?page=1&product_id=135003 reveals the same content our original URL with slightly different styling. What this second URL offers, though, is a way to select a certain page by changing the number after `page=`.

Now we can construct the URLs for all the pages we want to iterate through. We'll take the first 45 and assume that there are 99 reviews on each page putting us at nearly the full content of our 4500 or so reviews.

```{r}
pages_want <- 1:45
slack_full_urls <- str_c("https://www.capterra.com/gdm_reviews?page=", pages_want, "&product_id=135003", sep = "")
```


Now we can wrap `get_ratings_and_content` up into something that iterates through multiple pages. The maximum number of reviews on a page is 99, so we'll set our default `review_range` to that.

In `get_ratings_and_content` we included the page and rating number in the review file name as its unique identifier.



```{r}
get_multiple_pages <- function(urls, review_range = 1:99, ...) {
  out <- tibble()
  for (u in urls) {
    this_page <- get_ratings_and_content(u, review_range = review_range, ...)
    out <- out %>% 
      bind_rows(this_page)
  }
  return(out)
}
```


We'll write our results out to our same data directory.

```{r eval=FALSE}
all_reviews_slack <- 
  slack_full_urls %>% get_multiple_pages()
```


We could also read them all in from our directory and reconstitute the dataframe.

```{r, eval=FALSE}
make_files <- function() {
  a <- glue(data_dir, "page_{1:35}")
  out <- NULL
  
  for (i in 1:99) {
    b <- c(a %>% map_chr(str_c, "_rating_", i, ".rds", sep = ""))
    out <- c(out, b)
  }
  return(out)
}

fls <- make_files()

all_reviews_slack <- map_dfr(fls, read_rds) %>% 
  unnest(page_num) %>% 
  drop_na(content)
```


<br>


### Post-Processing

Next a few quick cleaning steps. We'll `clean_content` by removing anything with more than one space.

Content consists of sub-categories like "Pros," "Cons," and "Overall". We can split those each into their own columns with a few more regexes.

```{r}
clean_content <- function(t) {
  out <- t %>% 
    t %>% 
    str_replace_all("[ ]{2,}", "")   
  
  return(out)
}


split_pro_cons <- function(t) {
  out <- t %>% 
    rowwise() %>% 
    mutate(
      content = content %>% clean_content(),
      pros = str_extract(content, "(?<=Pros:).*?(?=Cons:)")
    )
  
  out <- out %>% 
    rowwise() %>% 
    mutate(
      cons = ifelse(str_detect(content, "Overall:"), 
                    str_extract(content, "(?<=Cons:).*?(?=Overall:)"),
                    str_extract(content, "(?<=Cons:).*")),
      overall = ifelse(str_detect(content, "Overall:"),
                       str_extract(content, "(?<=Overall:).*"),
                       NA_character_)
    )
 
  return(out)
}
```


We similarly want to split sub-ratings up into their own rows. When we scraped subratings concatenated all of them into one long string. 

For each long string, we'll extract all numbers and the corresponding name of the sub-rating the rating number belongs to.

If something goes wrong and there are more subrating numbers than names or vice versa, we'll alert ourselves of this by returning a tibble that just says "length_mismatch".

```{r}
split_subratings <- function(inp) {
  
  if (is.na(inp)) {
    out <- tibble(names = NA_character_, nums = NA_character_)
    return(out)
  } 
  
  nums <- inp %>% 
    str_extract_all("([0-9 ]+\\/[0-9 ]+)") %>% 
    map(str_replace_all, "[ ]+", "") %>% 
    as_vector()
  nums <- nums[nums != ""]
  
  names <- inp %>% 
    str_split("([0-9\\. ]+\\/[0-9 ]+)") %>% 
    as_vector()
  names <- names[names != ""]
  
  if (length(nums) != length(names)) {
    out <- tibble(names = "length_mismatch", nums = "length_mismatch")
    return(out)
  }
  
  out <- tibble(names = names, nums = nums)
  return(out)
}
```



Now we can take our raw reviews and get each sub-rating into a nested dataframe inside a new `sub_ratings_split` column, which can then be unnested

```{r, eval=FALSE}
reviews_with_subratings <- all_reviews_slack %>% 
  rowwise() %>% 
  mutate(
    sub_ratings_split = split_subratings(sub_ratings) %>% list()
  ) %>% 
  select(-sub_ratings) %>% 
  drop_na(content) %>% 
  filter(!content %in% c("", " ")) %>% 
  mutate(
    page_num = as.numeric(page_num)
  )

reviews_with_subratings_unnested <- 
  reviews_with_subratings %>% 
  unnest() %>% 
  rename(
    sub_rating_category = names,
    sub_rating_rating = nums
  ) %>% 
  arrange(page_num, review_num, sub_rating_category) 
```


Let's take a peek at our first 10 rows.

```{r}
reviews_with_subratings_unnested %>% 
  slice(1:10) %>% 
  kable()
```



<br>

## Extracting Opinions


The `monkeylearn` R package can get us most of the way there. Since we're working with a custom-trained extractor and classifier, the API response is slightly different from ones that the package is set up to handle. In particular, the package's handling of `NULL` values doesn't extend perfectly to this extractor and classifier, so we'll need to do a slight modification to change the missing values into values that we can unnest correctly.

If we get a `NULL` response from our classifier we'll replace it with `replacement_classifier` and likewise for `replacement_extractor`.

```{r}
replacement_classifier <- tribble(
  ~category_id, ~probability, ~label,
  NA_character_, NA_character_, NA_character_
) %>% list()


replacement_extractor <- tribble(
  ~count, ~tag, ~entity,
  NA_character_, NA_character_, NA_character_
) 
```

We'll use these inside the unnesting helpers below. They're different because of slighly different particularities to the API response.

```{r}
unnest_result_classifier <- function(df) {
  out <- df %>% 
    rowwise() %>% 
    mutate(
      res = ifelse(length(res)[[1]] == 0 || is.na(res), replacement_classifier, res) 
    ) %>% 
    unnest(res)
  
  return(out)
}

unnest_result_extractor <- function(df) {
  out <- df 
  df$res <- df$res %>% 
    map(dobtools::replace_x, replacement = replacement_extractor)
  
  out <- df %>% 
    unnest(res)
  
  return(out)
}

try_unnest_result_extractor <- safely(unnest_result_extractor)
try_unnest_result_classifier <- safely(unnest_result_classifier)
```


We wrap the usual `monkeylearn` package functions `monkey_classify` and `monkey_extract` in a trycatch using `purrr::safely` which returns a list of two things, one of which is always `NULL`; the result of the function and an error. If the function fails, we get the error and a `NULL` response and if it succeeds we get the response and a `NULL` error.

```{r}
get_classification_batch <- 
  safely(monkey_classify)

get_extraction_batch <- 
  safely(monkey_extract)
```


Rather than writing one extraction wrapper and one classifier wrapper, I combined them both into the same function below. We supply an `id` which can be either an extractor ID or a classifier ID set `type_of_problem` to either `"classification"` or `"extraction"` depending on the ID. (All classifier IDs begin with `"cl_"` and all extractor IDs begin with ``"ex_"`).

If any errors occur when we send text to the API, we log them in an error log that specifies where the error occurred and return that log along with the full response.

As of `monkeylearn` 2.0 it is possible to send batches of texts to the API and return a dataframe 


```{r}
write_batches <- function(df, id, dir, 
                          n_texts_per_batch,
                          start_row = 1,
                          unnest = FALSE,
                          write_out = TRUE, ...) {
  if (substr(id, 1, 3) == "cl_") {
    type_of_problem <- "classification"
  } else if (substr(id, 1, 3) == "ex_") {
    type_of_problem <- "extraction"
  } else {
    stop("Not a recognized classifier or extractor id.")
  }
  
  resp <- tibble()
  n_df_rows <- nrow(df)
  
  batch_start_row <- start_row
  batch_end_row <- batch_start_row + n_texts_per_batch
  
  error_log <- ""
  
  while(batch_start_row <= n_df_rows) {
    
    if (type_of_problem == "classification") {
      this_batch_nested <- get_classification_batch(df[batch_start_row:batch_end_row, ],
                                     col = content,
                                     classifier_id = id, 
                                     unnest = unnest)
      this_batch <- this_batch_nested$result %>% 
        try_unnest_result_classifier()
      
    } else if (type_of_problem == "extraction") {
      this_batch_nested <- get_extraction_batch(df[batch_start_row:batch_end_row, ],
                                     col = content,
                                     extactor_id = id, 
                                     unnest = unnest)
      
      this_batch <- this_batch_nested$result %>% 
        try_unnest_result_extractor()
    } 
    
    message(glue("Processed rows {batch_start_row} to {batch_end_row}."))
    
    if (is.null(this_batch_nested$error) && is.null(this_batch$error)) {
      if (write_out == TRUE) {
        write_csv(this_batch$result, 
                  glue("{dir}/{type_of_problem}_batches_rows_{batch_start_row}_to_{batch_end_row}.csv"))
      }
    
      resp <- resp %>% 
        bind_rows(this_batch$result)
      
    } else {
      error_log <- error_log %>% 
        c(glue("Error between rows {batch_start_row} and {batch_end_row}: 
               {c(this_batch_nested$error, this_batch$error)}"))
      
      message(error_log)
    }
    
    batch_start_row <- batch_start_row + n_texts_per_batch
    batch_end_row <- batch_start_row + n_texts_per_batch
    
    if (batch_end_row > n_df_rows) {
      batch_end_row <- n_df_rows
    }
  }
  
  out <- list(resp = resp, 
              error_log = error_log)
  
  return(out)
}
```







