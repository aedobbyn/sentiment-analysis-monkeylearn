---
title: "MonkeyLearn Sentiment Analysis"
output:
  html_document:
    keep_md: true
    toc: false
    theme: yeti
  github_document:
    toc: false
  pdf_document:
    keep_tex: true
    toc: false
---

In this post I'll detail how to use MonkeyLearn to conduct a seamless sentiment analysis of product reviews. I'll be examining reviews of Slack on the product review site Capterra.

The approach here will be to first scrape and tidy reviews and their associated ratings. Next, we'll feed each of the reviews to MonkeyLearn in order extract discrete opinion units from the text.  Finally, we'll use a custom-trained MonkeyLearn sentiment classifier to classify each opinion unit into its primary sentiment: Negative, Neutral, or Positive, as well as the category it fits into best (e.g., UI-UX, Pricing, Mobile, etc.).

The opinion unit approach gives us some more fine-grained control over what we're assigning sentiment to, since there can be multiple sentimetns in the same sentence. For instance, "I love Spring time but I hate the allergies that go along with it" would hopefully be broken into the units "I love Spring time" and "but I hate the allergies that go along with it" and assigned the sentiments Positive and Negative, respectively.


### Scraping the Data

First off we want to make sure that the website we're considering allows for scraping. We can consult the `robots.txt` file that typically lives at the top level of a website with the handy `robotstxt` package.

```{r setup, echo=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
               cache = TRUE, autodep = TRUE,     
               fig.width=12, fig.height=8,
               cache.extra = list())

options(knitr.table.format = 'html')
```


```{r}
library(here)
library(tidyverse)
library(rvest)
library(monkeylearn)
library(glue)
library(knitr)
library(dobtools)
library(tidytext)
```


```{r source_in, echo=FALSE, message=FALSE}
all_reviews_slack <- read_csv(here("data", "derived", "all_reviews_slack.csv"))
reviews_with_subratings_nested <- read_csv(here("data", "derived", "reviews_with_subratings_nested.rds"))
reviews_with_subratings_unnested <- read_csv(here("data", "derived", "capterra_slack_reviews_with_subratings_unnested.csv"))

data_dir <- glue(here::here("data", "raw"), "/")

dat <- 
  read_csv(here("data", "derived", "dat.csv")) 
```


```{r}
robotstxt::paths_allowed(
  domain = "capterra.com",
  path = "/",
  bot = "*"
)
```

Now that we know we're good to go, we can start scraping Capterra. I'll use the popular package `rvest` to do the scraping. `rvest` allows us to ask for content by a specific HTML tag or CSS class rather than grabbing all text on the page. We can find out which elements on the page to extract using the [SelectorGadget Chrome extension](). 

In this case, we'll want each review's overall rating, all of its sub-ratings, as well as the text, or content, of the review. What I'm calling sub-ratings are the reviews' ratings of aspects of the service like Customer Support, Ease of Use, and Value for Money. Like the overall rating, users have the option to give the aspects a rating out of 5.

We'll first save the URL where the Slack reviews appear

```{r}
slack_url <- "https://www.capterra.com/p/135003/Slack/"
```

and write quick helper for stripping out extra whitespace and newlines from our HTML soup.

```{r}
strip_whitespace_newlines <- function(t) {
  out <- t %>% 
    str_replace_all("\\n", " ") %>% 
    trimws() 
  
  return(out)
}
```


The original URL, https://www.capterra.com/p/135003/Slack/, loads 99 reviews of the total 4500+ at the time of scraping. To load more, the user would hit the "Show more reviews" button at the bottom of the page. This fetches more reviews from the server, but doesn't change the URL at all. That means that just using `rvest` and this URL, we can only scrape the first 99 reviews.

However, through a bit of investigation of the "Show more reviews" button in the Chrome inspector, I saw that inside the link's `a` tag is a `data-url` parameter set to `"/gdm_reviews?page=1&product_id=135003"`. Piecing that together with our original url, https://www.capterra.com/gdm_reviews?page=1&product_id=135003 reveals the same content our original URL with slightly different styling. What this second URL offers, though, is a way to select a certain page by changing the number after `page=`.

Now we can construct the URLs for all the pages we want to iterate through. We'll take the first 45 and assume that there are 99 reviews on each page putting us at nearly the full content of our 4500 or so reviews.

```{r}
pages_want <- 1:45
slack_full_urls <- str_c("https://www.capterra.com/gdm_reviews?page=", pages_want, "&product_id=135003", sep = "")
```


We can specify which of the 99 reviews on a page we want to scrape with from the review's number, contained in `#review-<review_number_here>` What `scrape_rating` will do is  `#review-{i} .overall-rating` We'll wrap the `scrape_rating` function in a trycatch so that we return an `NA` if something goes wrong rather than an error.

```{r}
scrape_rating <- function(url, i) {
  out <- url %>% 
    read_html() %>% 
    html_nodes(glue("#review-{i} .overall-rating")) %>% 
    html_text() %>% 
    strip_whitespace_newlines()
  
  if (length(out) == 0) {
    out <- NA_character_
  }
  
  return(out)
}

try_scrape_rating <- possibly(scrape_rating, otherwise = NA_character_)
```


Same deal for content and sub-ratings except we use a different selector and concatenate all of the ratings with `str_c(collapse = " ")` for now. We'll break those out later into a nested list column.


```{r, echo=FALSE}
scrape_sub_ratings <- function(url, i) {
  out <- url %>% 
    read_html() %>% 
    html_nodes(glue("#review-{i} .palm-one-half")) %>% 
    html_text() %>% 
    str_c(collapse = " ") %>% 
    strip_whitespace_newlines()
  
  if (length(out) == 0) {
    out <- NA_character_
  }
  
  return(out)
}

scrape_content <- function(url, i) {
  out <- url %>% 
    read_html() %>% 
    html_nodes(glue(".cell-review:nth-child({i}) .color-text")) %>% 
    html_text() %>% 
    strip_whitespace_newlines()
  
  if (length(out) == 0) {
    out <- NA_character_
  }
  
  return(out)
}
```




```{r}
try_scrape_sub_ratings <- possibly(scrape_sub_ratings, otherwise = NA_character_)
try_scrape_content <- possibly(scrape_content, otherwise = NA_character_)
```


```{r}
try_scrape_content(slack_full_urls[1], 42)
```


Now all that's left is to string these all together in the right order inside a function, `get_ratings_and_content`, that grabs a vector of reviews from a single page. We can keep track of which page we're scraping by extracting it straight from the URL itself. (If we use the original Slack URL which doesn't contain a page number, the `page` value gets an `NA`.)

In each iteration I go review by review here and grab both the rating and the review content before moving onto the next review to be absolutely sure that we're correctly matching rating and review. This approach is slower than grabbing all reviews and all ratings and matching them up afterward, but that could potentially get hairy if there are cases where we have more ratings than reviews on a page or vice versa.

Here we jitter the amount of wait time between each iteration using a uniform distribution around `sleep`, plus or minus half a second.

Throughout this post I'll be writing data out to small files in a directory so that if our loop fails somewhere along the way for any number of reasons, we won't lose all the data we've collected up to that point in the loop and can pick up again where we errored out on the first go-round. In this case, if we ask `get_ratings_and_content` to write what we've got so far, we create a new file in whatever `write_path` is set to.


```{r}
get_ratings_and_content <- function(url, review_range = 1:50,
                                    sleep = 1,
                                    write_out = TRUE, write_path = data_dir) {
  ifelse(sleep <= 1, 1, sleep)
  
  out <- tibble()
  page <- ifelse(str_detect(url, "page"), 
                 qdapRegex::ex_between(url, "page=", "&"),
                 NA_character_)
  
  for (i in review_range) {
    message(glue("Beginning scrape of page {page}, review {i}"))
    this_rating <- try_scrape_rating(url, i)
    
    this_sub_ratings <- try_scrape_sub_ratings(url, i)

    this_cont <- try_scrape_content(url, i)

    this_review <- tibble(
      rating = this_rating,
      sub_ratings = this_sub_ratings,
      content = this_cont,
      page_num = page,
      review_num = i
    ) 
    
    if (write_out == TRUE) {
      write_rds(this_review, path = glue(data_dir, "page_{page}_rating_{i}.rds"))
    }
    
    out <- out %>% 
      bind_rows(this_review)
    
    Sys.sleep(runif(1, sleep - 0.5, sleep + 0.5))
  }
  
  out <- out %>% 
    rowwise() %>% 
    mutate(
      rating_perc = ifelse(is.na(rating), NA_character_, 
                           parse(text = rating) %>% eval()) %>% as.character()
    ) %>% 
    select(page_num, review_num, rating, sub_ratings, rating_perc, content)
  
  return(out)
}

```



Let's give it a go.

```{r sample_ratings_scraped, eval=FALSE}
get_ratings_and_content(url = slack_full_urls[10], 
                        review_range = 1:3)
```


Now that we can grab everything we want from a single page, let's wrap `get_ratings_and_content` up into something that iterates through multiple pages. The maximum number of reviews on a page is 99, so we'll set our default `review_range` to that.

In `get_ratings_and_content` we included the page and rating number in the review file name as its unique identifier. `get_multiple_pages` will write our results out to our same data directory.


```{r}
get_multiple_pages <- function(urls, review_range = 1:99, ...) {
  out <- tibble()
  for (u in urls) {
    this_page <- get_ratings_and_content(u, review_range = review_range, ...)
    out <- out %>% 
      bind_rows(this_page)
  }
  return(out)
}
```


Let's scrape all of the pages!

```{r eval=FALSE}
all_reviews_slack <- 
  slack_full_urls %>% get_multiple_pages()
```

And now we can check out what we've got:

```{r}
all_reviews_slack %>% 
  head() %>% 
  kable()
```



We could also read them all in from our directory and reconstitute the dataframe.

```{r, eval=FALSE}
make_files <- function(page_range) {
  a <- glue(data_dir, "page_{page_range}")
  out <- NULL
  
  for (i in 1:99) {
    b <- c(a %>% map_chr(str_c, "_rating_", i, ".rds", sep = ""))
    out <- c(out, b)
  }
  return(out)
}

fls <- make_files(page_range = pages_want)

all_reviews_slack <- map_dfr(fls, read_rds) %>% 
  unnest(page_num) %>% 
  drop_na(content)
```


<br>


### Post-Processing

Next a few quick cleaning steps. Our review content often represents newlines by including a lot of extra whitespace. We'll `clean_content` by cleaning out multiple spaces.

```{r}
clean_content <- function(t) {
  out <- t %>% 
    t %>% 
    str_replace_all("[ ]{2,}", "")   
  
  return(out)
}
```



<!-- Content consists of sub-categories like "Pros," "Cons," and "Overall". We can split those each into their own columns with a few more regexes. -->

<!-- ```{r} -->
<!-- split_pro_cons <- function(t) { -->
<!--   out <- t %>%  -->
<!--     rowwise() %>%  -->
<!--     mutate( -->
<!--       content = content %>% clean_content(), -->
<!--       pros = str_extract(content, "(?<=Pros:).*?(?=Cons:)") -->
<!--     ) -->

<!--   out <- out %>%  -->
<!--     rowwise() %>%  -->
<!--     mutate( -->
<!--       cons = ifelse(str_detect(content, "Overall:"),  -->
<!--                     str_extract(content, "(?<=Cons:).*?(?=Overall:)"), -->
<!--                     str_extract(content, "(?<=Cons:).*")), -->
<!--       overall = ifelse(str_detect(content, "Overall:"), -->
<!--                        str_extract(content, "(?<=Overall:).*"), -->
<!--                        NA_character_) -->
<!--     ) -->

<!--   return(out) -->
<!-- } -->
<!-- ``` -->


Now let's clean up our subratings. When we scraped subratings we concatenated all of them into one long string because there wasn't an selector for each individual subrating. We want to split sub-ratings up into their own rows; for each long string, we'll extract all numbers and the corresponding name of the sub-rating the rating number belongs to (e.g. Value for Money 5/5) into a dataframe representing the same information in tidy format. We'll nest these in their own list column for later unnesting if we so choose. Rows without any sub-ratings get the appropriate NAs in both columns.

If something goes wrong and there are more subrating numbers than names or vice versa, we'll alert ourselves of this by returning a tibble that just says "length_mismatch".

```{r}
split_subratings <- function(inp) {
  
  if (is.na(inp)) {
    out <- tibble(names = NA_character_, nums = NA_character_)
    return(out)
  } 
  
  nums <- inp %>% 
    str_extract_all("([0-9 ]+\\/[0-9 ]+)") %>% 
    map(str_replace_all, "[ ]+", "") %>% 
    as_vector()
  nums <- nums[nums != ""]
  
  names <- inp %>% 
    str_split("([0-9\\. ]+\\/[0-9 ]+)") %>% 
    as_vector()
  names <- names[names != ""]
  
  if (length(nums) != length(names)) {
    out <- tibble(names = "length_mismatch", nums = "length_mismatch")
    return(out)
  }
  
  out <- tibble(names = names, nums = nums)
  return(out)
}
```



Now we can take our raw reviews and get each sub-rating into a nested dataframe inside a new `sub_ratings_split` column, which can then be unnested.

```{r, eval=FALSE}
reviews_with_subratings <- all_reviews_slack %>% 
  rowwise() %>% 
  mutate(
    sub_ratings_split = split_subratings(sub_ratings) %>% list()
  ) %>% 
  select(-sub_ratings) %>% 
  drop_na(content) %>% 
  filter(!content %in% c("", " ")) %>% 
  mutate(
    page_num = as.numeric(page_num)
  )

reviews_with_subratings_unnested <- 
  reviews_with_subratings %>% 
  unnest() %>% 
  rename(
    sub_rating_category = names,
    sub_rating_rating = nums
  ) %>% 
  arrange(page_num, review_num, sub_rating_category) 
```


Let's take a peek at our first 10 rows.

```{r}
reviews_with_subratings_unnested %>% 
  slice(1:10) %>% 
  kable()
```



<br>

Now onto the data processing! This is where MonkeyLearn's custom modules come in. They make it simple to send data their way for extraction or classification without having to build, train, test, and iterate on a model ourselves. The MonkeyLearn modules, custom-trained on our scraped data, even provide metrics like the precision, recall, and accuracy of each particular module.

<br>

## Extracting Opinions and Classifying Topics & Sentiment

First step is to extract opinion units from our review content. Each review will have one or many opinion units. What we want to end up with is one row per opinion unit, meaning multiple rows per original content line. Next, we send each opinion unit to the API to be assigned a a single sentiment (Positive, Negative, or Neutral) and classified into one or multiple topics. For topic classifications, we'll only worry about the "leaf" topics for these purposes.[^1]

The `monkeylearn` R package can get us most of the way there. `monkeylearn` exposes two functions for processing data: `monkey_extract` for extraction and `monkey_classify` for classification. They accept either a vector of texts or a dataframe and a named column where the texts live and return a dataframe relating each row of input to its outputs (either nested in a `res` column or unnested if `unnest` is TRUE). If unnested, that result will include a few columns including the extraction/classification label as well as the probability (confidence) MonkeyLearn assigns its classification/extraction We also need to supply an API key either as a string or (preferably) stored as an environment variable.

Since we're working with custom-trained extractors and classifiers, the API response is slightly different from responses that the package is set up to handle. In particular, the package's handling of `NULL` values doesn't extend perfectly to this extractor and classifier, so we'll need to do a slight modification to change the missing values into values that we can unnest correctly. For that reason, we'll set `unnest` to FALSE inside of `monkey_extract` and `monkey_classify` and do the unnesting ourselves.
<!-- The API returns zero-length vectors or `NULL`s for empty opinion units or when a unit wasn't classified into any categories. -->

If we get a `NULL` response from our classifier we'll replace it with `replacement_classifier` and likewise for `replacement_extractor`.

```{r}
replacement_classifier <- tribble(
  ~category_id, ~probability, ~label,
  NA_character_, NA_character_, NA_character_
) %>% list()


replacement_extractor <- tribble(
  ~count, ~tag, ~entity,
  NA_character_, NA_character_, NA_character_
) 
```

We'll use these replacements inside the unnesting helpers below. They're different because of slighly different particularities to the API response. [`dobtools::replace_x`](https://github.com/aedobbyn/dobtools/blob/master/R/replace_x.R) is just a generic "replace this NULL or 0L vector with whatever replacement I specify" function that is often useful for turning `NULL`s in API response data into `NA`s so that nested values can be unnested and tidied properly. (Incidentally, this was developed before the first version of the rOpenSci [`roomba`](https://github.com/ropenscilabs/roomba) package was released, which could have proved useful for tasks like this.)

```{r}
unnest_result_classifier <- function(df) {
  out <- df %>% 
    rowwise() %>% 
    mutate(
      res = ifelse(length(res)[[1]] == 0 || is.na(res), replacement_classifier, res) 
    ) %>% 
    unnest(res)
  
  return(out)
}

unnest_result_extractor <- function(df) {
  out <- df 
  df$res <- df$res %>% 
    map(dobtools::replace_x, replacement = replacement_extractor)
  
  out <- df %>% 
    unnest(res)
  
  return(out)
}

try_unnest_result_extractor <- safely(unnest_result_extractor)
try_unnest_result_classifier <- safely(unnest_result_classifier)
```


We wrap the usual `monkeylearn` package functions `monkey_classify` and `monkey_extract` in a trycatch using `purrr::safely` which returns a list of two things, one of which is always `NULL`; the result of the function and an error. If the function fails, we get the error and a `NULL` response and if it succeeds we get the response and a `NULL` error.

```{r}
get_classification_batch <- 
  safely(monkey_classify)

get_extraction_batch <- 
  safely(monkey_extract)
```


Now a function wrap up `monkey_classify` and `monkey_extract`. Rather than writing one extraction wrapper and one classifier wrapper, I combined them both into the same function below. We supply an `id` which can be either an extractor ID or a classifier ID set `type_of_problem` to either `"classification"` or `"extraction"` depending on the ID. (All classifier IDs begin with `"cl_"` and all extractor IDs begin with ``"ex_"`).

If any errors occur when we send text to the API, we log them in an error log that specifies where the error occurred and return that log along with the full response.

As of `monkeylearn` 2.0 it is possible to send batches of texts to the API and return a dataframe relating each input to its (often) multiple outputs. That allows us to include a `n_texts_per_batch` argument which we'll normally set to 200, the recommended maximum number of texts to be sent to the API at once. `monkeylearn` already takes care of rate limiting, so we don't need to put in any custom sleeps ourselves.


```{r}
write_batches <- function(df, id, dir, 
                          n_texts_per_batch,
                          start_row = 1,
                          unnest = FALSE,
                          write_out = TRUE, ...) {
  if (substr(id, 1, 3) == "cl_") {
    type_of_problem <- "classification"
  } else if (substr(id, 1, 3) == "ex_") {
    type_of_problem <- "extraction"
  } else {
    stop("Not a recognized classifier or extractor id.")
  }
  
  resp <- tibble()
  n_df_rows <- nrow(df)
  
  batch_start_row <- start_row
  batch_end_row <- batch_start_row + n_texts_per_batch
  
  error_log <- ""
  
  while(batch_start_row <= n_df_rows) {
    
    if (type_of_problem == "classification") {
      this_batch_nested <- get_classification_batch(df[batch_start_row:batch_end_row, ],
                                     col = content,
                                     classifier_id = id, 
                                     unnest = unnest)
      this_batch <- this_batch_nested$result %>% 
        try_unnest_result_classifier()
      
    } else if (type_of_problem == "extraction") {
      this_batch_nested <- get_extraction_batch(df[batch_start_row:batch_end_row, ],
                                     col = content,
                                     extactor_id = id, 
                                     unnest = unnest)
      
      this_batch <- this_batch_nested$result %>% 
        try_unnest_result_extractor()
    } 
    
    message(glue("Processed rows {batch_start_row} to {batch_end_row}."))
    
    if (is.null(this_batch_nested$error) && is.null(this_batch$error)) {
      if (write_out == TRUE) {
        write_csv(this_batch$result, 
                  glue("{dir}/{type_of_problem}_batches_rows_{batch_start_row}_to_{batch_end_row}.csv"))
      }
    
      resp <- resp %>% 
        bind_rows(this_batch$result)
      
    } else {
      error_log <- error_log %>% 
        c(glue("Error between rows {batch_start_row} and {batch_end_row}: 
               {c(this_batch_nested$error, this_batch$error)}"))
      
      message(error_log)
    }
    
    batch_start_row <- batch_start_row + n_texts_per_batch
    batch_end_row <- batch_start_row + n_texts_per_batch
    
    if (batch_end_row > n_df_rows) {
      batch_end_row <- n_df_rows
    }
  }
  
  out <- list(resp = resp, 
              error_log = error_log)
  
  return(out)
}
```




We'll take the same tack of storing each of these results in its own file (this time a CSV) in a user-specified directory. We can wrap `write_batches` in a couple helper functions with extraction- or classification-relevant results so that we don't need to specify them every time.

```{r}
write_extraction_batches <- function(df, n_texts_per_batch = 200, 
                                     dir = opinion_batches_dir, ...) {
  write_batches(df, id = extractor_id, n_texts_per_batch = n_texts_per_batch,
                dir = dir, ...)
}

write_classification_batches <- function(df, n_texts_per_batch = 200, 
                                         dir = topic_batches_dir, ...) {
  write_batches(df, id = topic_id, n_texts_per_batch = n_texts_per_batch,
                dir = dir, ...)
}
```


We'll also need an easy way to gather up all of our result files up into a single dataframe. `gather_batches` will take a directory, create a vector of all the files that exist in it, `read_csv` in all of those files into one long list of dataframes, and then index into every element in that list to bind all of the dataframes together, rowwise.

```{r}
gather_batches <- function(dir, end_row) {
  
  fls <- fs::dir_ls(dir)
  
  list_o_batches <- 
    map(fls, 
        read_csv)
  
  out <- list_o_batches %>% 
    modify_depth(2, as.character) %>%
    bind_rows()
  
  return(out)
}
```


Cool, now we've set up everything we need to process our data and reconsitute the result. As a quick refresher, the flow here is that we'll take our scraped data, send it to the extractor to extract multiple opinion units per review, and then send each of those opinion units to the classifier to receive its multiple classifications per opinion unit.

```{r, eval=FALSE}
# Write opinion units
reviews_with_subratings_nested %>% 
  write_extraction_batches()

# Gather opinion units
opinion_batches_extracted <- 
  gather_batches(dir = opinion_batches_dir) %>% 
  rename(probability_unit = probability)

# Classify opinion units
opinion_batches_extracted %>% 
  write_classification_batches()

# Gather classifications
dat <- 
  gather_batches(dir = topic_batches_dir) %>% 
  rename(probability_sentiment = probability)
```

We could even have saved the `gather_batches` steps if everything went well with our `write_{}_batches` functions. Assigning that output to a variable also allows us to inspect the `error_log` to see if all went well.


<br>

## Analysis

Let's take a look at our data.

```{r}
dat %>% 
  head() %>% 
  kable()
```

There are only three possible sentiments for an opinion unit to have,

```{r}
dat$sentiment %>% factor() %>% levels()
```

so we can assign a number to each type of sentiment. 

```{r}
dat <- dat %>% 
  rowwise() %>% 
  mutate(
    sentiment_num = switch(sentiment, 
                           "Negative" = -1,
                           "Neutral" = 0,
                           "Positive" = 1)
  ) %>% 
  ungroup()
```


What about categories?

```{r}
dat$category %>% factor() %>% levels()
```

We can see there are some categories labeled "None". It's tough to know how to interpret these, so we can filter out these rows in a new `dat_clean` dataframe. We'll also filter out low-probability sentiments and categories -- anything that the classifier is less than 55% sure is classified correctly.

```{r}
probability_cutoff <- 0.55

dat_clean <-
  dat %>% 
  filter(!is.na(probability_unit) & !is.na(probability_unit) & 
           category != "None" &
           probability_sentiment > probability_cutoff & probability_unit > probability_cutoff)
```



What is the interaction between the two main things of interest here, category and sentiment? Let's get a summary of the mean sentiment (based off of our numerical representation of sentiment) for opinion units that have been classified into each cateogry.

```{r}
sentiment_by_category <- 
  dat_clean %>% 
  group_by(category) %>% 
  summarise(
    mean_sentiment = mean(sentiment_num)
  ) %>% 
  arrange(mean_sentiment)

sentiment_by_category %>% 
  kable()
```

`r sentiment_by_category[which(sentiment_by_category$mean_sentiment == min(sentiment_by_category$mean_sentiment)), ]$category` gets the lowest overall sentiment, whereas `r sentiment_by_category[which(sentiment_by_category$mean_sentiment == max(sentiment_by_category$mean_sentiment)), ]$category` gets the highest.

```{r}
ggplot(sentiment_by_category) +
  geom_bar(aes(fct_reorder(category, mean_sentiment), mean_sentiment), stat = "identity") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Sentiment by Category") +
  labs(x = "Category", y = "Sentiment")
```





### Down to the word level

We can go even deeper into these reviews by splitting them into their individual words. The `tidytext` package is fantastic for this purpose. We'll use its `unnest_tokens` function to get a long dataframe of all words and then clean them up a bit by filtering out `stop_words` (a dataset included in the package) like "and" and "the". My helper [`dobtools::find_nums()`](https://github.com/aedobbyn/dobtools/blob/master/R/find_nums.R) mutates on a couple columns: one for whether the word in question is a number (i.e. can be converted to type numeric) and one for whether the word contains a number. If `is_num` is TRUE, then `contains_num` is also always TRUE.

```{r}
dat_tokens_unnested <- 
  dat_clean %>% 
  unnest_tokens(word, content) %>% 
  anti_join(stop_words, "word") %>%
  dobtools::find_nums() %>% 
  filter(contains_num == FALSE) %>% 
  add_count(word) %>% 
  rename(
    n_words_total = n
  )

```


The `tidytext` package also includes a `sentiments` dataset which we can join on our words to get a classification of the words' sentiment in three different lexicons as well as its score on a scale of -5 (negative) to 5 (positive). (`?tidytext::sentiments` for a full explanation of the dataset.) For example:

```{r}
sentiments %>% filter(word == "yucky")
```

and 

```{r}
sentiments %>% filter(word == "yummy")
```

(Don't ask me why yummy is more positive than yucky is negative `emo::ji("laughing")`.) Anyway, let's join this on our dataset by "word".


```{r}
dat_tokens_unnested <-  
  dat_tokens_unnested %>% 
  select(-is_num, -contains_num) %>%
  left_join(tidytext::sentiments %>% 
              rename(word_sentiment = sentiment,
                     score_sentiment = score), 
            by = "word")
```



Do the words that make up an opinion unit have a significatn effect on the sentiment it's assigned by MonkeyLearn?

```{r}
lm(data = dat_tokens_unnested %>% drop_na(sentiment_num, score_sentiment), 
   sentiment_num ~ score_sentiment) %>% 
  summary() %>% 
  tidy()
```

Yes -- words with more positive sentiments tend to occur in more positive opinion units.


As a control, we can check whether words that appear at the beginning of the alphabet tend to get higher sentiment scores. (I don't know of a reason to suspect this might be the case which is why I'm treating it as a control but maybe there is a psycholinguist out there who can set me straight?)

We'll extract just the first letter of the word and assign it the number of the alphabet from 1 to 24.

```{r}
dat_tokens_unnested_first_letter <- 
  dat_tokens_unnested %>% 
  rowwise() %>% 
  mutate(
    first_letter = substr(word, 1, 1),
    first_letter_num = which(letters == first_letter)
  ) 

```

```{r, echo=FALSE}
ggplot(dat_tokens_unnested_first_letter) +
  geom_smooth(aes(first_letter_num, sentiment_num),
              colour = "blue") +
  geom_smooth(aes(first_letter_num, score_sentiment),
              colour = "red") +
  labs(x = "First letter", y = "Sentiment") +
  ggtitle("Relationship between a word's first letter on its sentiment") +
  theme_bw()
```


What about the statistical relationship?

```{r}
lm(data = dat_tokens_unnested_first_letter %>% drop_na(first_letter_num, score_sentiment), 
   first_letter_num ~ score_sentiment) %>% 
  summary() %>% 
  tidy()
```

Not significant, as we might expect.


[^1]: What I mean by that is: this particular topic classifier has a tree-like structure where each leaf belong to a single parent. MonkeyLearn first classifies each text into a top-level supercategory, one of: App-Software, Service, or Other. Once a text is classified at this first level, it then gets classified into one or more children in that supercategory. For instance, the children of App-Software are: Characteristics, Devices-Web, and Features. Finally, each of these has its own children. Take Characteristics: its children, which are leaf categories or terminal nodes for the classifier are Ease of Use, Integrations, Performance-Quality-Reliability, and UI-UX. This means that if a text doesn't appear to reference App-Software, the grandparent of UI-UX, it won't have a chance of being classified as UI-UX. This scheme of course doesn't preclude a text from being classified under multiple terminal nodes.



